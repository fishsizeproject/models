[
  {
    "objectID": "Bayesian_inla.html",
    "href": "Bayesian_inla.html",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "",
    "text": "Fisheries catches through time are usually correlated. Catch in one year is not independent of the catch in the previous year. In CPUE standardisation models we should ideally account for this autocorrelation. The GLM based models don’t really do that, despite being a popular approach. Therefore here we will develop and apply a different model which accounts for spatial and temporal correlation in our catches.\nIn this model we will estimate model parameters using approximate Bayesian inference as implemented in the INLA package. The approximate Bayesian inference runs much faster than full inference using MCMC approach and with INLA package it can be applied to a wide range of distributions. For a basic introduction into the Bayesian approach and why it is useful, check out these slides.\nThe slides and the model, together with a detailed description of all the modelling steps have been presented in the last lecture of the CPUE standardisation course, so we recommend you watch the lecture before applying the model. Also, before proceeding with CPUE standardisation, make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage.\nThe model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling."
  },
  {
    "objectID": "Bayesian_inla.html#model-code",
    "href": "Bayesian_inla.html#model-code",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "Model code",
    "text": "Model code\nThe main model code is available in this R markdown script and is demonstrated using this dataset from 22 years of commercial pikeperch (Sander lucioperca) catches in the Baltic Sea. You can download the model and modify the script according to your needs. Before you run the model you will need to install all the R packages, as is explained in this R script.\nTo look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted CPUE and its uncertainty through time, like in the plot below."
  },
  {
    "objectID": "Bayesian_inla.html#application-of-the-model",
    "href": "Bayesian_inla.html#application-of-the-model",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "Application of the model",
    "text": "Application of the model\nWe are currently working on applying this model to CPUE standardisation of the long term data set from the Curonian Lagoon and Kaunas Water Reservoir in Lithuania. Stay tuned for more outputs. If you are interested to learn more, please contact us as lydekaipaliepus@gamtc.lt"
  },
  {
    "objectID": "curonian_mizer.html",
    "href": "curonian_mizer.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "detecting_size.html",
    "href": "detecting_size.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "GLM_standardisation.html",
    "href": "GLM_standardisation.html",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "",
    "text": "Before proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared the courses and scripts for data exploration, which you can find on our CPUE standardisation course webpage.\nOne commonly used approach for CPUE data standardisation applies generalized linear models (GLM). Here we model all variables that could impact our catches and extract annual deviations and their uncertainty. From this we can plot a standardized time series of population abundance.\nIn this set of slides you will find main points about GLM based CPUE standardisation.\nFor examples on how this method has been applied to other stocks, see references here and here"
  },
  {
    "objectID": "GLM_standardisation.html#model-code",
    "href": "GLM_standardisation.html#model-code",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "Model code",
    "text": "Model code\nWe have developed GLM based models for CPUE standardisation of five fish species in the Curonian Lagoon and Kaunas Water reservoir (Lithuania). To use our models you first need to convert your dataset into a format where each row corresponds to a unique entry indicating all catches per gear, mesh size, length, season, fishing trip or other variables you want to include in the model. You can use this code to covert your data table into a suitable format.\nTo apply our GLM based standardisation model you can use this code where you will apply generalized linear models with Tweedie distribution and assess important predictor variables. Once you extract annual residuals and associated uncertainty you can plot the time series, as in the image below.\n EGLE: need to add Rmd scripts"
  },
  {
    "objectID": "GLM_standardisation.html#application-of-the-model",
    "href": "GLM_standardisation.html#application-of-the-model",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "Application of the model",
    "text": "Application of the model\nTo learn more about the application of this CPUE standardisation model to Kaunas Water Reservoir fish populations, you can read our publication in journal Fishes.\nYou can also watch this conference talk to learn more about this analysis.\nOnce you conduct your CPUE standardisation, you can use the standardised values in surplus production models, as explained on this website."
  },
  {
    "objectID": "growth_change.html",
    "href": "growth_change.html",
    "title": "Detecting changes in fish growth through time",
    "section": "",
    "text": "Although temperature can have large impacts on fish growth, it can be hard to detect it in size and age data, due to inherent noise and variability in growth. To separate temperature signals from other growth variation we need model that will estimate temperature impacts on fish size at age. Here we develop such model and apply it to the long term fish growth observation dataset in Drūkšiai Lake in Lithuania. Drūkšiai lake presents a unique system because during 1994-2010 the lake served as nuclear power plant cooling lake and was heated by nearly 2C. Since 2010 the power plant has been closed and the lake temperature returned to natural levels. Nature Research Centre has been conducting long term monitoring of fish growth has been conducted in Drūkšiai Lake, and now we can use these datasets to estimate how temperature changes affected growth in five fish species.\n Left: Drūkšiai Lake, LithuaniaRight: Water temperature timeseries"
  },
  {
    "objectID": "growth_change.html#model-code",
    "href": "growth_change.html#model-code",
    "title": "Detecting changes in fish growth through time",
    "section": "Model code",
    "text": "Model code\nThe model is in R environment and is applies Bayesian mixed models of length as a function of age, age squared, lifetime average temperature and the interaction between lifetime average temperature and age. The model code and output can be found here. The model estimates the intercept and slope of temperature effect on size at age. In this way we assess whether all ages growth faster or slower (sign and magnitude of the intercept) and whether temperature impacts are different for young versus older ages (sign and magnitude of the slope). You can see model predictions in the figure below."
  },
  {
    "objectID": "growth_change.html#application-of-the-model",
    "href": "growth_change.html#application-of-the-model",
    "title": "Detecting changes in fish growth through time",
    "section": "Application of the model",
    "text": "Application of the model\nThis model and its results have been presented at the ICES annual conference as an in September 2022 - see video here.\nIt also was presented at the 151st American Fisheries Society Annual Meeting in November 2021 - see video here\nIf you would like to suggest or implement new model modifications and publish them, please contact us at lydekaipaliepus@gamtc.lt\nPerch drawing by Amy Rose Coghlan"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models for sustainable inland fisheries",
    "section": "",
    "text": "This is a webpage of models developed during the project “Advanced models, citizen science and big data for sustainable food production and ecological services of inland aquatic ecosystems”. The project has received funding from European Regional Development Fund (project No 01.2.2-LMT-K-718-02-0006) under grant agreement with the Research Council of Lithuania (LMTLT).\nThis page and models presented here have been created by Asta Audzijonyte, Carl Smith, Catarina Silva, Egle Jakubavičiūtė, Dalia Grendaitė, Max Lindmark, Freddie Heather, Steve Midway and Gustav Delius.\nIf you have questions and suggestions please contact us at lydekaipaliepus@gamtc.lt\nStart exploring models\nGo back to the project website."
  },
  {
    "objectID": "jabba_models.html",
    "href": "jabba_models.html",
    "title": "Surplus production models",
    "section": "",
    "text": "JABBA is a Bayesian State-Space Surplus Production Model framework developed by Winker et al. 2018.\nIt uses catch and relative abundance time series and requires prior information on a) the resilience parameter r (intrinsic rate of population increase), b) carrying capacity K and c) the relative initial biomass at the beginning of the time series.\nYou can find all information in the developers’ [vignette]"
  },
  {
    "objectID": "jabba_models.html#model-code",
    "href": "jabba_models.html#model-code",
    "title": "Surplus production models",
    "section": "Model code",
    "text": "Model code\nHere is an example code of how we used JABBA for modeling five species in the Curonian Lagoon and Kaunas water reservoir (Lithuania).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEGLE: need to add Rmd scripts"
  },
  {
    "objectID": "jabba_models.html#application-of-the-model",
    "href": "jabba_models.html#application-of-the-model",
    "title": "Surplus production models",
    "section": "Application of the model",
    "text": "Application of the model\nTo learn more about surplus production modeling to Curonian Lagoon and Kaunas Water Reservoir fish stocks and results that could be of interest to managers, you can read this and this summary or watch this conference talk."
  },
  {
    "objectID": "ML_fish_size.html",
    "href": "ML_fish_size.html",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "",
    "text": "Here we present a simple approach to develop models for estimating fish size classes from images. We have prepared a scripts for data pre-processing and machine learning model development, which you can find on our Online course webpage.\nIn this publication you will find a detailed description of the method and a pilot case-study where we demonstrate potential use for estimating fish size classes from images without a speficied reference object. You can also find all the scripts used in the framework in our Github page."
  },
  {
    "objectID": "ML_fish_size.html#model-code",
    "href": "ML_fish_size.html#model-code",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "Model code",
    "text": "Model code\nTo use our models it is very important that fish size classes in your dataset are identified correctly, otherwise your model will not be very useful. You can upload images (in JPEG or PNG format) to your Google drive by size class (i.e. fish images of each size class per folder), following this directory structure:\ndataset\n|__ class5\n    |______ 100080576_f52e8ee070_n.PNG\n    |______ 14167534527_781ceb1b7a_n.PNG\n    |______ ...\n|__ class10\n    |______ 10043234166_e6dd915111_n.PNG\n    |______ 1426682852_e62169221f_m.PNG\n    |______ ...\n|__ class15\n    |______ 102501987_3cdb8e5394_n.PNG\n    |______ 14982802401_a3dfb22afb.PNG\n    |______ ...\n|__ ...\n\nTo apply our model you can use this code:\n\n\n\nOpen In Colab"
  },
  {
    "objectID": "ML_fish_size.html#application-of-the-model",
    "href": "ML_fish_size.html#application-of-the-model",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "Application of the model",
    "text": "Application of the model\nYou can also watch this talk for the “Online seminar: Recreational angling effort and engagement in the digital age” to learn more about this model."
  },
  {
    "objectID": "ML_fish_species.html",
    "href": "ML_fish_species.html",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "",
    "text": "Here we present a scalable user-friendly artificial intelligence (AI) framework to develop fish species identification models. We have prepared a course and scripts for data pre-processing, image processing and machine learning model development, which you can find on our Online course webpage.\nIn this publication in the journal Sustainability you will find a detailed description of the framework, a pilot case-study where we demonstrate potential use for recreational fisheries research, a summary of the knowledge gained from the case study application and an outline of the main challenges and potential future development. You can also find all the scripts used in the framework in our Github page.\n Open-source modular framework for large scale image storage, handling, annotation and automatic classification"
  },
  {
    "objectID": "ML_fish_species.html#model-code",
    "href": "ML_fish_species.html#model-code",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "Model code",
    "text": "Model code\nTo use our models it is very important that fish species (or ecotypes or any other classes you want your model to identify) in your dataset are identified correctly, otherwise your model will not be very useful (you might have heard the expression “garbage in - garbage out”). You can upload images (in JPEG or PNG format) to your Google drive by species (i.e. one species per folder - please make sure these are correctly identified), following this directory structure:\ndataset\n|__ perch\n    |______ 100080576_f52e8ee070_n.PNG\n    |______ 14167534527_781ceb1b7a_n.PNG\n    |______ ...\n|__ striped_bass\n    |______ 10043234166_e6dd915111_n.PNG\n    |______ 1426682852_e62169221f_m.PNG\n    |______ ...\n|__ trout\n    |______ 102501987_3cdb8e5394_n.PNG\n    |______ 14982802401_a3dfb22afb.PNG\n    |______ ...\n|__ ...\n\nTo apply our model you can use this code:"
  },
  {
    "objectID": "ML_fish_species.html#application-of-the-model",
    "href": "ML_fish_species.html#application-of-the-model",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "Application of the model",
    "text": "Application of the model\nYou can also watch this talk for the “Online workshop: Machine learning, fishing and citizen science” and this talk for the “Online seminar: Recreational angling effort and engagement in the digital age” to learn more about this model."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Overview of models",
    "section": "",
    "text": "We have developed and applied a range of statistical and mechanistic models, which we used in our studies and are happy to share with others. We hope these models will be useful for fisheries research and management. The models are available for free non-commercial use, but please reference the original source (publications or this website).\n\n\nScientific monitoring and artisanal, commercial or recreational fish catch data is often used to assess population status and trends, but such data are usually complex and require careful standardisation. There are several approaches that can be used for CPUE standardisation and we present three potential models of increasing complexity."
  },
  {
    "objectID": "models.html#surplus-production-models-for-stock-assessments",
    "href": "models.html#surplus-production-models-for-stock-assessments",
    "title": "Overview of models",
    "section": "Surplus production models for stock assessments",
    "text": "Surplus production models for stock assessments\nSurplus production (SP) models are commonly used to assess data-poor fish stocks and are based on time series of catches and population abundance index (such as standardised CPUE time series, from the models above). These models assume that fish population abundance depends on its regeneration rate, carrying capacity and catches. SP models have been successfully applied to many stocks and, despite their simple assumptions, often perform surprisingly well, assuming the population abundance and catch time series are reliable."
  },
  {
    "objectID": "models.html#fish-growth-models",
    "href": "models.html#fish-growth-models",
    "title": "Overview of models",
    "section": "Fish growth models",
    "text": "Fish growth models\nFish growth and therefore body sizes are highly variable, as it depends on temperature, food availability and many other processes. Understanding and modelling how and why growth can change is therefore an important part of fisheries research."
  },
  {
    "objectID": "models.html#machine-learning-models",
    "href": "models.html#machine-learning-models",
    "title": "Overview of models",
    "section": "Machine learning models",
    "text": "Machine learning models\nMachine learning (ML) enables rapid analyses of large image and datasets and is an important step to facilitate citizen science driven data collection techniques. During our project we have developed two machine learning models for fish species and size identification."
  },
  {
    "objectID": "models.html#satellite-data-analysis",
    "href": "models.html#satellite-data-analysis",
    "title": "Overview of models",
    "section": "Satellite data analysis",
    "text": "Satellite data analysis\nRemote sensing and satellite data is now being collected on high temporal and spatial resolution. However, it is often challenging to access and process these data. We have developed some models and tools to aid with satellite based water surface temperature and chlorophyll A data acquisition and analysis."
  },
  {
    "objectID": "models.html#size-based-ecosystem-models",
    "href": "models.html#size-based-ecosystem-models",
    "title": "Overview of models",
    "section": "Size based ecosystem models",
    "text": "Size based ecosystem models\nSize based community and ecosystem models represent a powerful tool to explore potential outcomes of different fisheries management strategies, species interactions, climate change and a lot more. In this project we are developing a size based model for the Curonian lagoon ecosystem.\n\n\n\n\n\n\n\n14. Sized based models for Curonian lagoon and Baltic Sea  Learn about the potential climate change impacts in the Baltic Sea, explore a basic Curonian lagoon model and understand key principles of size based modelling, as implemented in a R package mizer."
  },
  {
    "objectID": "mqmf.html",
    "href": "mqmf.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "satelite_chla.html",
    "href": "satelite_chla.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "satelite_temp.html",
    "href": "satelite_temp.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "scripts/BayesianINLA.html",
    "href": "scripts/BayesianINLA.html",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "",
    "text": "Note that we are using a data file that has already been explored and modified to remove outliers. Read the introduction to the model to see where to find the original data file and explanation on data exploration"
  },
  {
    "objectID": "scripts/BayesianINLA.html#data-exploration",
    "href": "scripts/BayesianINLA.html#data-exploration",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Data exploration",
    "text": "Data exploration\n\nCheck outliers\nThe plots below suggest that there are no outliers in the dataset\n\n\n\n\n\n\n\nNormality and homogeneity of variance\nThe plots below show that as catches and effort increase, so does the variance. This suggest a departure from the homogeneity of variance."
  },
  {
    "objectID": "scripts/BayesianINLA.html#fit-increasingly-complex-models",
    "href": "scripts/BayesianINLA.html#fit-increasingly-complex-models",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Fit increasingly complex models",
    "text": "Fit increasingly complex models\nWe start with an intercept only model. Catch is modelled as a function of year, whereas ‘rw1’ part imposes a temporal trend. We fit a gamma distribution using INLA. A gamma distribution is strictly positive (no zeros) and skewed (like our catch data)\n\n# Create a formula\nf1 <- Catch ~ + f(Year, model = \"rw1\")\n\n#fit a model\nI1 <- inla(f1, \n           control.compute = list(dic = TRUE), #estimate dic for model comparison\n           family = \"Gamma\",\n           data = zan)\n\n# Plot the time(year) trend\nYearsm <- I1$summary.random$Year\nFit1   <- I1$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,1), mar = c(5,5,2,2), cex.lab = 1.5)\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1))\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\n\n\n\nThe result above shows that if we only look at one trend, we see decreasing catches through time. But effort also might have changed, so we need to include effort in the model.\n\n#create a formula with effort\nf2 <- Catch ~ Effort + f(Year, model = \"rw1\")\n\n#fit a model\nI2 <- inla(f2, \n           control.compute = list(dic = TRUE),\n           family = \"Gamma\",\n           data = zan)\n#compare two models using a criterion similar to AIC\nround(I1$dic$dic,0) #4179\n\n[1] 4179\n\nround(I2$dic$dic,0) #3532 <- including Effort improves fit\n\n[1] 3532\n\n#And plot the trend \nYearsm <- I2$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-0.2, 0.2) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\n\n\n\nNow the trend is clearly different. This is because we are not including effort in the model, which is important. However, we also need to account for potentially different trends across months. So now we will nest month within a year\n\n#create a formula\nf3 <- Catch ~ Effort + \n  f(Year, \n    model = \"rw1\") +\n  f(Mon, \n    model = \"rw1\", cyclic = TRUE)  \n\n#fit the model\nI3 <- inla(f3, \n           control.predictor = list(compute = TRUE),\n           control.compute = list(dic = TRUE),\n           family = \"gamma\",\n           data = zan)\n\n#compare with the previous model \nround(I2$dic$dic,0) #3525\n\n[1] 3532\n\nround(I3$dic$dic,0) #3203 <- including month improves fit\n\n[1] 3203\n\n#plot the trend, but this time we plot it for years and months\nFit3 <- I3$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)\nYearsm   <- I3$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\nMonsm <- I3$summary.random$Mon\nplot(Monsm[,1:2], type='l',\n     xlab = 'MonthInYear', \n     ylab = '',\n     ylim = c(-4, 4) )\nabline(h=0, lty=3)\nlines(Monsm[, c(1, 4)], lty=2)\nlines(Monsm[, c(1, 6)], lty=2)\n\n\n\n\nWe can see that catches change a lot during the year and are low in summer months. So we definitely need to include month in the model. Since we have several stations, we probably also need to include stations in the model.\n\n# create a formula with station as a random term\nf4 <- Catch ~ Effort + \n  f(Year, \n    model = \"rw1\") +\n  f(Mon, \n    model = \"rw1\", cyclic = TRUE) +\n  f(fStn, model = \"iid\") \n\n#fit the model \nI4 <- inla(f4, \n           control.predictor = list(compute = TRUE),\n           control.compute = list(dic = TRUE),\n           family = \"Gamma\",\n           data = zan)\n\n#compare with the previous model \nround(I3$dic$dic,0) #3203\n\n[1] 3203\n\nround(I4$dic$dic,0) #2977 <- including station improves fit\n\n[1] 2977\n\n#Plot trend across years and months \nFit4 <- I4$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)\nYearsm   <- I4$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\nMonsm   <- I4$summary.random$Mon\nplot(Monsm[,1:2], type='l',\n     xlab = 'MonthInYear', \n     ylab = 'Random walk trend',\n     ylim = c(-3, 3) )\nabline(h=0, lty=3)\nlines(Monsm[, c(1, 4)], lty=2)\nlines(Monsm[, c(1, 6)], lty=2)"
  },
  {
    "objectID": "scripts/BayesianINLA.html#explore-the-best-model",
    "href": "scripts/BayesianINLA.html#explore-the-best-model",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Explore the best model",
    "text": "Explore the best model\nNow we look at the Bayesian residuals\n\n\n\n\n\n\n\n\n\n\n\nThese residuals look ok. However, if we fit residuals versus effort we see a strange non linear pattern, which suggests that the model does not fully fit. We will then plot it for each station and find that there seem to be different residual and therefore likely also CPUE trends in different stations.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nTo solve this problem we make station a fixed effect and add an interaction with effort to capture non-linearity. If different stations have different catch trends this interaction should be included. So we define a yet another model with Effort * fStn interaction and fit it.\n\n#define the model\nf5 <- Catch ~ Effort * fStn + \n  f(Year, \n    model = \"rw1\") +\n  f(Mon, \n    model = \"rw1\",\n        cyclic = T)\n#fit the model \nI5 <- inla(f5, \n           control.predictor = list(compute = TRUE),\n           control.compute = list(config = TRUE, dic = TRUE),\n           family = \"Gamma\",\n           control.family = list(link = \"log\"),\n           data = zan)\n\n#compare wit the previous model \nround(I4$dic$dic,0) #2977 \n\n[1] 2977\n\nround(I5$dic$dic,0) #2889 <- big improvement in fit...\n\n[1] 2889\n\n#plot trends \nFit5 <- I5$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)\nYearsm   <- I5$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\nMonsm   <- I5$summary.random$Mon\nplot(Monsm[,1:2], type='l',\n     xlab = 'MonthInYear', \n     ylab = 'Random walk trend',\n     ylim = c(-3, 3) )\nabline(h=0, lty=3)\nlines(Monsm[, c(1, 4)], lty=2)\nlines(Monsm[, c(1, 6)], lty=2)\n\n\n\n\nNow we look at the residuals of our new model\n\n# Get the fitted values and Pearson residuals\nN     <- nrow(zan)\nmu2   <- I5$summary.fitted.values[1:N,\"mean\"] \nr     <- I5$summary.hyperpar[\"Precision parameter for the Gamma observations\", \"mean\"]\nVarY2 <- mu2^2 / r\nE2    <- (zan$Catch - mu2) / sqrt(VarY2)\n\n# Plot residuals versus fitted values.\npar(mfrow = c(1, 1))\nplot(x = mu2, \n     y = E2,\n     xlab = \"Fitted values\",\n     ylab = \"Pearson residuals\")\nabline(h = 0, lty = 2, col = 1)\n\n\n\n# Plot residuals versus station\nboxplot(E2 ~ Station, \n        ylab = \"Pearson residuals\",\n        data = zan)\nabline(h = 0, lty = 2)\n\n\n\n# Year\nboxplot(E2 ~ Year, \n        ylab = \"Pearson residuals\",\n        data = zan)\nabline(h = 0, lty = 2)\n\n\n\n# Month\nboxplot(E2 ~ Month, \n        ylab = \"Pearson residuals\",\n        data = zan)\nabline(h = 0, lty = 2)\n\n\n\n# Residuals versus effort\nzan$E2 <- E2\n\nresplot2 <- ggplot() +\n  geom_point(data = zan, alpha = 0.4, size = 2,\n             aes(y = E2 ,  \n                 x = Effort)) +\n  geom_smooth(data = zan,                    \n              aes(y = E2, \n                  x = Effort)) +\n  xlab(\"Effort\") + ylab(\"Pearson residuals\") +\n  theme(text = element_text(size = 12), legend.position=\"none\") +\n  theme(axis.text.x = element_text(size = 11, angle = 45, hjust = 0.9)) +\n  My_theme +\n  geom_hline(yintercept = 0, col = 2)\nresplot2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe final plot shows that the significant nonlinear pattern in residuals is mostly gone (uncertainty ranges include zero)."
  },
  {
    "objectID": "scripts/BayesianINLA.html#plot-model-outputs",
    "href": "scripts/BayesianINLA.html#plot-model-outputs",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Plot model outputs",
    "text": "Plot model outputs\nFirst we plots the overall temporal trend in years and months\n\n# Plot temporal effects\np1 <- bind_rows(\n  I5$summary.random$Year %>%\n    select(Year = 1, mean = 2, lcl = 4, ucl = 6) %>%\n    mutate(Model = \"rw1\")\n) %>%\n  ggplot(aes(x = Year, y = mean, ymin = lcl, ymax = ucl)) +\n  geom_ribbon(alpha = 0.2, fill = \"forestgreen\") +\n  geom_line(colour = \"forestgreen\") + My_theme +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n             color = \"firebrick2\", size=0.4) +\n  ggtitle(\"Year\") +\n  theme(legend.position = \"none\")\nggplotly(p1)\n\n\n\n\np2 <- bind_rows(\n  I5$summary.random$Mon %>%\n    select(Mon = 1, mean = 2, lcl = 4, ucl = 6) %>%\n    mutate(Model = \"rw2\")\n) %>%\n  ggplot(aes(x = Mon, y = mean, ymin = lcl, ymax = ucl)) +\n  geom_ribbon(alpha = 0.2, fill = \"dodgerblue2\") +\n  geom_line(colour = \"dodgerblue2\") + My_theme +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n             color = \"firebrick2\", size=0.4) +\n  ggtitle(\"Month within Year\") +\n  theme(legend.position = \"none\")\nggplotly(p2)"
  },
  {
    "objectID": "scripts/BayesianINLA.html#plot-catch-versus-effort",
    "href": "scripts/BayesianINLA.html#plot-catch-versus-effort",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Plot catch versus effort",
    "text": "Plot catch versus effort\nHere we plot the model output where we show catch as a function of effort for each station. Note that these predictions will be shown for one selected year (year 2002 in this case) and month (September), whereas data scatterplot shows the full data."
  },
  {
    "objectID": "scripts/BayesianINLA.html#plot-cpue-through-time",
    "href": "scripts/BayesianINLA.html#plot-cpue-through-time",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Plot CPUE through time",
    "text": "Plot CPUE through time\nNow we plot CPUE through time for a selected station (station 1) and selected effort level (20K), for all months"
  },
  {
    "objectID": "scripts/Hilsha.html",
    "href": "scripts/Hilsha.html",
    "title": "R code for hilsa (Tenualosa ilisha) CPUE data: zero inflated models",
    "section": "",
    "text": "Note that we are using a data file that has already been explored and modified to remove outliers. Read the introduction to the model to see where to find the original data file and explanation on data exploration"
  },
  {
    "objectID": "scripts/Hilsha.html#plot-model-outputs-and-parameters",
    "href": "scripts/Hilsha.html#plot-model-outputs-and-parameters",
    "title": "R code for hilsa (Tenualosa ilisha) CPUE data: zero inflated models",
    "section": "Plot model outputs and parameters",
    "text": "Plot model outputs and parameters\nWe plot model outputs to show predictions on how catches will depend on the number of days spent fishing and gillnet lengths.\n\n\n\n\n\n\n\n\nWe can also plot how catches depend on the trip length during different months and in different fishing areas."
  },
  {
    "objectID": "scripts/Hilsha.html#plot-model-parameter-estimates-in-a-publication-format",
    "href": "scripts/Hilsha.html#plot-model-parameter-estimates-in-a-publication-format",
    "title": "R code for hilsa (Tenualosa ilisha) CPUE data: zero inflated models",
    "section": "Plot model parameter estimates in a publication format",
    "text": "Plot model parameter estimates in a publication format\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n\n\n\n \nNB GLM (Hilsha)\n\n\nCoeffcient\nLog-Mean\nConf. Int (95%)\nP-value\n\n\n(Intercept)\n2.49\n2.06 – 2.92\n<0.001\n\n\nArea [LowerMeghna]\n-0.11\n-0.48 – 0.26\n0.556\n\n\nArea [LowerPadma]\n-0.44\n-0.89 – 0.01\n0.056\n\n\nYrMonth [Oct92]\n0.38\n0.10 – 0.66\n0.007\n\n\nYrMonth [Sep92]\n-0.08\n-0.35 – 0.20\n0.596\n\n\nNlength\n0.00\n-0.00 – 0.00\n0.174\n\n\nTripDays\n0.41\n0.33 – 0.49\n<0.001\n\n\nObservations\n266\n\n\nR2 conditional / R2 marginal\nNA / 0.960"
  },
  {
    "objectID": "surplus-production.html",
    "href": "surplus-production.html",
    "title": "Surplus production models",
    "section": "",
    "text": "Blah spm info\nHere is a link to the shiny app"
  },
  {
    "objectID": "temperature_growth.html",
    "href": "temperature_growth.html",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "",
    "text": "Fish growth is strongly dependent on temperature. In warmer conditions fish often growth faster as juveniles, mature at smaller sizes and are smaller as adults. This is known as the temperature size rule, as defined by David Atkinson in 1994.\n\n\n\n\n\nHowever, the mechanisms behind this growth remain unclear. In this model we propose that temperature size rule (TSR) emerges in response to both physiological changes (faster metabolism and intake) and growth and reproduction optimisation to changes in mortality. We develop a physiologically structured life history optimisation model and explore a range of parameters and scenarios under which TSR is likely to emerge. For a brief introduction into the model you can watch these slides or watch this conference talk."
  },
  {
    "objectID": "temperature_growth.html#model-code",
    "href": "temperature_growth.html#model-code",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "Model code",
    "text": "Model code\nThe model is written in Microsoft Excel and R environment. All model code and details are available on this GitHub repository. The main model Excel file can also be downloaded here. In this Excel file you can modify temperature response parameters and their size dependency, optimise the model using Solver option built in Excel and see the resulting growth and reproduction curves, as shown in the figure below."
  },
  {
    "objectID": "temperature_growth.html#application-of-the-model",
    "href": "temperature_growth.html#application-of-the-model",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "Application of the model",
    "text": "Application of the model\nThe open access publication presenting this model is published in a special issue of The Biological Bulletin. If you want to learn more about the role of temperature and oxygen on fish growth, please also check out this overview publication in the same special issue.\nThis model is easy to modify and explore, and you are welcome to use it in your research and teaching. If you would like to suggest or implement new model modifications and publish them, please contact us at lydekaipaliepus@gamtc.lt\n Vendace drawing by Amy Rose Coghlan"
  },
  {
    "objectID": "to_do_list.html",
    "href": "to_do_list.html",
    "title": "TO DO list",
    "section": "",
    "text": "Why is our webiste https://fishsizeproject.github.io/models/ not updated once I push code updates? I can see them locally, but not online"
  },
  {
    "objectID": "to_do_list.html#section",
    "href": "to_do_list.html#section",
    "title": "TO DO list",
    "section": "",
    "text": "Asta: get chlorophyl A image"
  },
  {
    "objectID": "von-bertalanffy.html",
    "href": "von-bertalanffy.html",
    "title": "von Bertalanffy Growth models",
    "section": "",
    "text": "blah von Bert info\nHere is a link to the shiny app"
  },
  {
    "objectID": "Zero_inflated_model.html",
    "href": "Zero_inflated_model.html",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "",
    "text": "Most fisheries datasets (scientific, commercial, recreational) have a lot of zero catches. These are fishing trips were no fish was caught. These zero catches are important but we need to fit appropriate models to accommodate them. Here we are introducing a model for zero inflated data. This model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling.\nFor a basic introduction into the model and data, check out these slides. However, if you want to use the model we strongly recommend that you watch at least part 4 of our CPUE standardisation course, where the model and approach were presented in more detail.\nBefore proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage."
  },
  {
    "objectID": "Zero_inflated_model.html#model-code",
    "href": "Zero_inflated_model.html#model-code",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "Model code",
    "text": "Model code\nThe main model code is available in this R markdown script and is demonstrated using this dataset. You can download the model and modify the script according to your needs. To look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted catches as a function of fishing time (or other estimate of effort) and month or season, like in the plot below."
  },
  {
    "objectID": "Zero_inflated_model.html#application-of-the-model",
    "href": "Zero_inflated_model.html#application-of-the-model",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "Application of the model",
    "text": "Application of the model\nTo better understand this model and its applications, we strongly recommend that you go through our CPUE standardisation course material, where we discuss different models and their strengths in greater detail. The course also explains how to simulate new datasets using estimated model parameters to assess the probability of obtaining as many zero entries as you have in your dataset (example output of these simulations is shown in the plot below)."
  }
]