[
  {
    "objectID": "Bayesian_inla.html",
    "href": "Bayesian_inla.html",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "",
    "text": "Fisheries catches through time are usually correlated. Catch in one year is not independent of the catch in the previous year. In CPUE standardisation models we should ideally account for this autocorrelation. The GLM based models don’t really do that, despite being a popular approach. Therefore here we will develop and apply a different model which accounts for spatial and temporal correlation in our catches.\nIn this model we will estimate model parameters using approximate Bayesian inference as implemented in the INLA package. The approximate Bayesian inference runs much faster than full inference using MCMC approach and with INLA package it can be applied to a wide range of distributions. For a basic introduction into the Bayesian approach and why it is useful, check out these slides.\nThe slides and the model, together with a detailed description of all the modelling steps have been presented in the last lecture of the CPUE standardisation course, so we recommend you watch the lecture before applying the model. Also, before proceeding with CPUE standardisation, make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage.\nThe model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling."
  },
  {
    "objectID": "Bayesian_inla.html#model-code",
    "href": "Bayesian_inla.html#model-code",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "Model code",
    "text": "Model code\nThe main model code is available in this R markdown script and is demonstrated using this dataset from 22 years of commercial pikeperch (Sander lucioperca) catches in the Baltic Sea. You can download the model and modify the script according to your needs. Before you run the model you will need to install all the R packages, as is explained in this R script.\nTo look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted CPUE and its uncertainty through time, like in the plot below."
  },
  {
    "objectID": "Bayesian_inla.html#application-of-the-model",
    "href": "Bayesian_inla.html#application-of-the-model",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "Application of the model",
    "text": "Application of the model\nWe are currently working on applying this model to CPUE standardisation of the long term data set from the Curonian Lagoon and Kaunas Water Reservoir in Lithuania. Stay tuned for more outputs. If you are interested to learn more, please contact us as lydekaipaliepus@gamtc.lt"
  },
  {
    "objectID": "curonian_mizer.html",
    "href": "curonian_mizer.html",
    "title": "ENGLISH TITLE MIZER",
    "section": "",
    "text": "ENGLISH TITLE MIZER\nAbout the model (english)\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "GLM_standardisation.html",
    "href": "GLM_standardisation.html",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "",
    "text": "Before proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared the courses and scripts for data exploration, which you can find on our CPUE standardisation course webpage.\nOne commonly used approach for CPUE data standardisation applies generalized linear models (GLM). Here we model all variables that could impact our catches and extract annual deviations and their uncertainty. From this we can plot a standardized time series of population abundance.\nIn this set of slides you will find main points about GLM based CPUE standardisation.\nFor examples on how this method has been applied to other stocks, see references here and here"
  },
  {
    "objectID": "GLM_standardisation.html#model-code",
    "href": "GLM_standardisation.html#model-code",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "Model code",
    "text": "Model code\nWe have developed GLM based models for CPUE standardisation of five fish species in the Curonian Lagoon and Kaunas Water reservoir (Lithuania). To use our models you first need to convert your dataset into a format where each row corresponds to a unique entry indicating all catches per gear, mesh size, length, season, fishing trip or other variables you want to include in the model. You can use this code to covert your data table into a suitable format.\nTo apply our GLM based standardisation model you can use this code where you will apply generalized linear models with Tweedie distribution and assess important predictor variables. Once you extract annual residuals and associated uncertainty you can plot the time series, as in the image below. To look at the model code and outputs without having to run the code, you can click here."
  },
  {
    "objectID": "GLM_standardisation.html#application-of-the-model",
    "href": "GLM_standardisation.html#application-of-the-model",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "Application of the model",
    "text": "Application of the model\nTo learn more about the application of this CPUE standardisation model to Kaunas Water Reservoir fish populations, you can read our publication in journal Fishes.\nYou can also watch this conference talk to learn more about this analysis.\nOnce you conduct your CPUE standardisation, you can use the standardised values in surplus production models, as explained on this website."
  },
  {
    "objectID": "growth_change.html",
    "href": "growth_change.html",
    "title": "Detecting changes in fish growth through time",
    "section": "",
    "text": "Although temperature can have large impacts on fish growth, it can be hard to detect it in size and age data, due to inherent noise and variability in growth. To separate temperature signals from other growth variation we need model that will estimate temperature impacts on fish size at age. Here we develop such model and apply it to the long term fish growth observation dataset in Drūkšiai Lake in Lithuania. Drūkšiai lake presents a unique system because during 1994-2010 the lake served as nuclear power plant cooling lake and was heated by nearly 2C. Since 2010 the power plant has been closed and the lake temperature returned to natural levels. Nature Research Centre has been conducting long term monitoring of fish growth has been conducted in Drūkšiai Lake, and now we can use these datasets to estimate how temperature changes affected growth in five fish species."
  },
  {
    "objectID": "growth_change.html#model-code",
    "href": "growth_change.html#model-code",
    "title": "Detecting changes in fish growth through time",
    "section": "Model code",
    "text": "Model code\nThe model is in R environment and is applies Bayesian mixed models of length as a function of age, age squared, lifetime average temperature and the interaction between lifetime average temperature and age. The model code and output can be found here. The model estimates the intercept and slope of temperature effect on size at age. In this way we assess whether all ages growth faster or slower (sign and magnitude of the intercept) and whether temperature impacts are different for young versus older ages (sign and magnitude of the slope). You can see model predictions in the figure below."
  },
  {
    "objectID": "growth_change.html#application-of-the-model",
    "href": "growth_change.html#application-of-the-model",
    "title": "Detecting changes in fish growth through time",
    "section": "Application of the model",
    "text": "Application of the model\nThis model and its results have been presented at the ICES annual conference as an in September 2022 - see video here.\nIt also was presented at the 151st American Fisheries Society Annual Meeting in November 2021 - see video here\nIf you would like to suggest or implement new model modifications and publish them, please contact us at lydekaipaliepus@gamtc.lt\nPerch drawing by Amy Rose Coghlan"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models for sustainable inland fisheries",
    "section": "",
    "text": "This is a webpage of models developed during the project “Advanced models, citizen science and big data for sustainable food production and ecological services of inland aquatic ecosystems”. The project has received funding from European Regional Development Fund (project No 01.2.2-LMT-K-718-02-0006) under grant agreement with the Research Council of Lithuania (LMTLT).\nThis page and models presented here have been created by Asta Audzijonyte, Carl Smith, Catarina Silva, Egle Jakubavičiūtė, Dalia Grendaitė, Max Lindmark, Freddie Heather, Steve Midway and Gustav Delius.\nIf you have questions and suggestions please contact us at lydekaipaliepus@gamtc.lt\nStart exploring models\nGo back to the project website."
  },
  {
    "objectID": "jabba_models.html",
    "href": "jabba_models.html",
    "title": "Surplus production models",
    "section": "",
    "text": "JABBA is a Bayesian State-Space Surplus Production Model framework developed by Winker et al. 2018.\nIt uses catch and relative abundance time series and requires prior information on a) the resilience parameter r (intrinsic rate of population increase), b) carrying capacity K and c) the relative initial biomass at the beginning of the time series.\nYou can find all information in the developers’ [vignette]"
  },
  {
    "objectID": "jabba_models.html#model-code",
    "href": "jabba_models.html#model-code",
    "title": "Surplus production models",
    "section": "Model code",
    "text": "Model code\nHere is an example code of how we used JABBA for modeling five species in the Curonian Lagoon and Kaunas water reservoir (Lithuania)."
  },
  {
    "objectID": "jabba_models.html#application-of-the-model",
    "href": "jabba_models.html#application-of-the-model",
    "title": "Surplus production models",
    "section": "Application of the model",
    "text": "Application of the model\nTo learn more about surplus production modeling to Curonian Lagoon and Kaunas Water Reservoir fish stocks and results that could be of interest to managers, you can read this and this summary or watch this and this conference talks."
  },
  {
    "objectID": "ML_fish_size.html",
    "href": "ML_fish_size.html",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "",
    "text": "Here we present a simple approach to develop models for estimating fish size classes from images. We have prepared a scripts for data pre-processing and machine learning model development, which you can find on our Online course webpage.\nIn this publication you will find a detailed description of the method and a pilot case-study where we demonstrate potential use for estimating fish size classes from images without a speficied reference object. You can also find all the scripts used in the framework in our Github page."
  },
  {
    "objectID": "ML_fish_size.html#model-code",
    "href": "ML_fish_size.html#model-code",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "Model code",
    "text": "Model code\nTo use our models it is very important that fish size classes in your dataset are identified correctly, otherwise your model will not be very useful. You can upload images (in JPEG or PNG format) to your Google drive by size class (i.e. fish images of each size class per folder), following this directory structure:\ndataset\n|__ class5\n    |______ 100080576_f52e8ee070_n.PNG\n    |______ 14167534527_781ceb1b7a_n.PNG\n    |______ ...\n|__ class10\n    |______ 10043234166_e6dd915111_n.PNG\n    |______ 1426682852_e62169221f_m.PNG\n    |______ ...\n|__ class15\n    |______ 102501987_3cdb8e5394_n.PNG\n    |______ 14982802401_a3dfb22afb.PNG\n    |______ ...\n|__ ...\n\nTo apply our model you can use this code:\n\n\n\nOpen In Colab"
  },
  {
    "objectID": "ML_fish_size.html#application-of-the-model",
    "href": "ML_fish_size.html#application-of-the-model",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "Application of the model",
    "text": "Application of the model\nYou can also watch this talk for the “Online seminar: Recreational angling effort and engagement in the digital age” to learn more about this model."
  },
  {
    "objectID": "ML_fish_species.html",
    "href": "ML_fish_species.html",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "",
    "text": "Here we present a scalable user-friendly artificial intelligence (AI) framework to develop fish species identification models. We have prepared a course and scripts for data pre-processing, image processing and machine learning model development, which you can find on our Online course webpage.\nIn this publication in the journal Sustainability you will find a detailed description of the framework, a pilot case-study where we demonstrate potential use for recreational fisheries research, a summary of the knowledge gained from the case study application and an outline of the main challenges and potential future development. You can also find all the scripts used in the framework in our Github page.\nOpen-source modular framework for large scale image storage, handling, annotation and automatic classification"
  },
  {
    "objectID": "ML_fish_species.html#model-code",
    "href": "ML_fish_species.html#model-code",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "Model code",
    "text": "Model code\nTo use our models it is very important that fish species (or ecotypes or any other classes you want your model to identify) in your dataset are identified correctly, otherwise your model will not be very useful (you might have heard the expression “garbage in - garbage out”). You can upload images (in JPEG or PNG format) to your Google drive by species (i.e. one species per folder - please make sure these are correctly identified), following this directory structure:\ndataset\n|__ perch\n    |______ perch_fig1.PNG\n    |______ another_photo.PNG\n    |______ ...\n|__ striped_bass\n    |______ photo_titles_do_not_matter.PNG\n    |______ another_striped_bass_photo.PNG\n    |______ ...\n|__ trout\n    |______ 1268952.PNG\n    |______ or_any_random_name.PNG\n    |______ ...\n|__ ...\n\nTo apply our model you can use this code:"
  },
  {
    "objectID": "ML_fish_species.html#application-of-the-model",
    "href": "ML_fish_species.html#application-of-the-model",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "Application of the model",
    "text": "Application of the model\nYou can also watch this talk for the “Online workshop: Machine learning, fishing and citizen science” and this talk for the “Online seminar: Recreational angling effort and engagement in the digital age” to learn more about this model."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Overview of models",
    "section": "",
    "text": "We have developed and applied a range of statistical and mechanistic models, which we used in our studies and are happy to share with others. We hope these models will be useful for fisheries research and management. The models are available for free non-commercial use, but please reference the original source (publications or this website).\n\n\nScientific monitoring and artisanal, commercial or recreational fish catch data is often used to assess population status and trends, but such data are usually complex and require careful standardisation. There are several approaches that can be used for CPUE standardisation and we present three potential models of increasing complexity."
  },
  {
    "objectID": "models.html#surplus-production-models-for-stock-assessments",
    "href": "models.html#surplus-production-models-for-stock-assessments",
    "title": "Overview of models",
    "section": "Surplus production models for stock assessments",
    "text": "Surplus production models for stock assessments\nSurplus production (SP) models are commonly used to assess data-poor fish stocks and are based on time series of catches and population abundance index (such as standardised CPUE time series, from the models above). These models assume that fish population abundance depends on its regeneration rate, carrying capacity and catches. SP models have been successfully applied to many stocks and, despite their simple assumptions, often perform surprisingly well, assuming the population abundance and catch time series are reliable."
  },
  {
    "objectID": "models.html#fish-growth-models",
    "href": "models.html#fish-growth-models",
    "title": "Overview of models",
    "section": "Fish growth models",
    "text": "Fish growth models\nFish growth and therefore body sizes are highly variable, as it depends on temperature, food availability and many other processes. Understanding and modelling how and why growth can change is therefore an important part of fisheries research."
  },
  {
    "objectID": "models.html#machine-learning-models",
    "href": "models.html#machine-learning-models",
    "title": "Overview of models",
    "section": "Machine learning models",
    "text": "Machine learning models\nMachine learning (ML) enables rapid analyses of large image and datasets and is an important step to facilitate citizen science driven data collection techniques. During our project we have developed two machine learning models for fish species and size identification."
  },
  {
    "objectID": "models.html#satellite-data-analysis",
    "href": "models.html#satellite-data-analysis",
    "title": "Overview of models",
    "section": "Satellite data analysis",
    "text": "Satellite data analysis\nRemote sensing and satellite data is now being collected on high temporal and spatial resolution. However, it is often challenging to access and process these data. We have developed some models and tools to aid with satellite based water surface temperature and chlorophyll A data acquisition and analysis."
  },
  {
    "objectID": "models.html#size-based-ecosystem-models",
    "href": "models.html#size-based-ecosystem-models",
    "title": "Overview of models",
    "section": "Size based ecosystem models",
    "text": "Size based ecosystem models\nSize based community and ecosystem models represent a powerful tool to explore potential outcomes of different fisheries management strategies, species interactions, climate change and a lot more. In this project we are developing a size based model for the Curonian lagoon ecosystem.\n\n\n\n\n\n\n\n13. Sized based models for Curonian lagoon and Baltic Sea  Learn about the potential climate change impacts in the Baltic Sea, explore a basic Curonian lagoon model and understand key principles of size based modelling, as implemented in a R package mizer."
  },
  {
    "objectID": "satelite_chla.html",
    "href": "satelite_chla.html",
    "title": "Extracting and analysing optical water quality data from satellites",
    "section": "",
    "text": "Water quality is an important issue in many human activities. Traditional methods to gain knowledge on water quality are accurate, but expensive and time consuming. Therefore, they do not cover all the water bodies and are usually taken just in one point of a lake. Yet, in larger water bodies differences of water quality between different stations might be large.\nSatellite data can help to overcome the scarcity of water quality data. Using satellites we can obtain more frequent data across space and time. When it comes to using satellite data for water quality studies, it is important to remember that satellites monitor water optical features. In freshwater ecosystems there are three main optically active substances: chlorophyll-a pigment found in algae and cyanobacteria, suspended matter and coloured dissolved organic matter (also called yellow substances).\nIn this document we provide user friendly tools to retrieve biophysical data on lake class, describing its water quality. The method uses spectral features of water obtained through European Space Agency’s (ESA) Copernicus Sentinel-2 satellites. The method is based on machine learning random forest algorithm and assigns the lake to one of the four classes:\n\nClear class. Such lakes have water transparency is >2 m in medium deep and deep lakes and >1.3 m in shallow lakes, low concentrations of optically active substances (they do not affect water spectral features). Lakes in this class usually also have good ecological status as defined in the EU Water Framework Directive.\nModerate class. Water transparency is still good or high, but chlorophyll-a concentration is between 7 and 20 mg/m^3 (higher than in clear class), indicating light algal and cyanobacterial blooms.\nChla-dominated class. Water transparency is <2 m in medium deep and deep lakes and <1.3 m in shallow lakes, the concentration of chlorophyll-a is often > 20 mg/m^3. Such lakes usually have bad or poor status.\nTurbid class. Water transparency is <2 m in medium deep and deep lakes and < 1.3 m in shallow lakes, but concentration of chlorophyll-a is similar to that in the moderate class (between 7 and 20 mg/m^3). Low water transparency is more likely related to higher concentrations of coloured dissolved matter.\n\nFurther details about the model and its validation are described in the open access paper by Grendaitė and Stonevičius (2022). The model accuracy on test set varied among classes: 27% for Turbid class, 70% for Chla-dominated class, 81% for Moderate class and 85% for the Clear class.\nThe code presented here adapts and applies the aforementioned model to any other lake coordinates. It first retrieves Sentinel-2 data using Google Earth engine (GEE). Then the data are filtered and class models applied."
  },
  {
    "objectID": "satelite_chla.html#model-code",
    "href": "satelite_chla.html#model-code",
    "title": "Extracting and analysing optical water quality data from satellites",
    "section": "Model code",
    "text": "Model code\nTo help you extract and process temperature data we have two user friendly tools. The main code to access and download the data from GEE is written in Python and is available through Google Colaboratory environment. Google Colaboratory is a free online resource which already has Python libraries installed, so you can run the scripts easily, even if you do not have programming skills. To access the satellite data collections through Google Earth Engine, you need to have a Google account(if you do not have Google account, you can sign up here. Then use your Google account to register for the Google Earth Engine, which will ask you to authorise your account.\nNext you can go to the model scripts on Google Colaboratory using the link below \nWhen you open the Google Colab notebook, make a copy of it in your own Google Drive (Select File and then Make a copy in Drive). You can edit your copy of the notebook, if you need, but don’t forget to save. Further instructions on how to use the notebook are found in the notebook itself.\nTo extract water quality (optical) data you will need to create and upload a csv file with codes, names and coordinates of you sites, like in this example file. You will also need to upload the auxiliary files with lake class classification models, which you can download here (but make sure you unzip the files before uploading them to the Google Colaboratory). The downloaded lake class data will look like this example file. In this example we only extract data for three lakes (three rows in the coordinate example file), but you can add as many lakes (rows) as you need. However, remember not to select coordinates close to the shore (ideally more than 50 meters) and don’t put points too close to each other, as the resolution of the data is 100 meters.\nOnce you have the file with satellite data retrieved for the selected coordinates, you can use this R markdown script to plot your results, like in the image below. In this code we also compare satellite data with in situ monitoring data in Lithuanian lakes. If you want to look at the results of this script without having to run it, you can see it here"
  },
  {
    "objectID": "satelite_chla.html#application-of-the-model",
    "href": "satelite_chla.html#application-of-the-model",
    "title": "Extracting and analysing optical water quality data from satellites",
    "section": "Application of the model",
    "text": "Application of the model\nThe model to assess lake class data could be used widely to monitor water quality across large scales and at high temporal resolution. As Europe is implementing its Water Framework Directive and has set the mission to Restore our ocean and waters wide scale availability of lake quality data is important. The code and algorithms presented here can be also used for various research projects, where scientists need to correlate biological characteristics to lake quality. The algorithm to groups lake classes has been developed for temperate lakes, and further work should be done to test how well it applies to lakes in, for example, tropical or arid regions."
  },
  {
    "objectID": "satelite_temp.html",
    "href": "satelite_temp.html",
    "title": "Extracting and analysing lake surface temperature data from satellites",
    "section": "",
    "text": "Water surface temperature (ST) can be obtained from Landsat mission data. The Landsat mission dates back to 1972, but ST recording are only available since the launch of Landsat 4 satellite in 1984, as this satellite had a thermal infrared sensor (TIRS) that detects long wavelengths of light emitted by the Earth. The intensity of these wavelengths depends on surface temperature.\nCurrently there are two Landsat satellites operating in Earth’s orbit - Landsat 8 (from March 2013) and Landsat 9 (from October 2021). The easiest way to access these data is through Google Earth engine (GEE), which now integrates an algorithm, which produces image files representing surface temperature (in Kelvin) for each Landsat data pixel. This algorithm was created at the Rochester Institute of Technology (RIT) and the NASA Jet Propulsion Laboratory (JPL) in cooperation with USGS software engineers (Landsat 4-7 and Landsat 8-9 product description). The algorithm uses reflectance data from multiple Landsat bands, TIRS band and auxiliary elevation and atmospheric data to derive the final surface temperature estimates. The data comes at 100 m resolution level, which means that it could be applied to even relatively small lakes. Landsat data are provided for latitudes up to 76 degrees. To learn more about Landsat 8 data collection see the GEE link."
  },
  {
    "objectID": "satelite_temp.html#model-code",
    "href": "satelite_temp.html#model-code",
    "title": "Extracting and analysing lake surface temperature data from satellites",
    "section": "Model code",
    "text": "Model code\nTo help you extract and process temperature data we have two user friendly tools. The main code to access and download the data from GEE is written in Python and is available through Google Colaboratory environment. Google Colaboratory is a free online resource which already has Python libraries installed, so you can run the scripts easily, even if you do not have programming skills. To access the satellite data collections through Google Earth Engine, you need to have a Google account(if you do not have Google account, you can sign up here. Then use your Google account to register for the Google Earth Engine, which will ask you to authorise your account.\nNext you can go to the model scripts on Google Colaboratory using the link below \nWhen you open the Google Colab notebook, make a copy of it in your own Google Drive (Select File and then Make a copy in Drive). You can edit your copy of the notebook, if you need, but don’t forget to save. Further instructions on how to use the notebook are found in the notebook itself. To extract surface temperature data you will need to create and upload a csv file with codes, names and coordinates of you sites, like in this example file. The dowloaded satellite data will look like this example file. In this example we only extract data for two lakes (two rows), but you can add as many lakes or sampling points in one lake (rows) as you need. However, remember not to select coordinates close to the shore (ideally more than 50 meters) and don’t put points too close to each other, as the resolution of the data is 100 meters.\nOnce you have the file with satellite data retrieved for the selected coordinates, you can use this R markdown script to filter cloudy data and remove potentially deteriorated data from Landsat 7 timeseries (from 2018 to 2022). If you want to look at the results of this script without having to run it, you can see it here"
  },
  {
    "objectID": "satelite_temp.html#application-of-the-model",
    "href": "satelite_temp.html#application-of-the-model",
    "title": "Extracting and analysing lake surface temperature data from satellites",
    "section": "Application of the model",
    "text": "Application of the model\nTo assess the quality of the satellite temperature data we will compare it with the monitoring data from 12 Lithuanian lakes, available from the Lithuanian Hydrometeorological Service (meteo.lt). We expect small differences because in situ monitoring points are close to the shore, whereas for satellite data we chose coordinates in the central part of lake. Because satellite data points very close to the shore (50 meters or less) will be influenced by the coastal surfaces, vegetation and terrain, it is important to only use points that are sufficiently far from the shore. Nevertheless, we can see that the correlation is very high with correlation coefficients usually higher than 0.9"
  },
  {
    "objectID": "surplus-production.html",
    "href": "surplus-production.html",
    "title": "Surplus production models",
    "section": "",
    "text": "Surplus production models are widely used in assessments of data limited fisheries stocks. These models are based on the assumption that stock abundance is largely determined by fishing, population regeneration or growth rate (r) and population carrying capacity (K), which represents the unfished biomass level. If we knew the r and K exactly, and if we had good data on catches through time, we could estimate population abundance pretty well. However, since r and K are not known, we use time series of catches and population abundance index (e.g. standardised catch per unit effort data) to estimate most likely values of r and K, assess population status and make future projections. This means that in order to apply surplus production models to our stocks we need information about:\n\ntime series of catches,\ntime series of standardised CPUE or other measure of population abundance index,\nsome preliminary guesses about r and K and initial population depletion level, or how close to the carrying capacity our population was at the start of our time series.\n\nAlso, because surplus production models estimate model parameters from available time series, they work best if the time series has sufficient contrast in catches and population abundance. If catches and CPUE remained relatively stable through time, then parameter and population status estimates will have large uncertainty ranges.\nThe main parameter, determining population resilience to fishing is the population regeneration rate r. We usually do not r for our species and population, but we can look up estimates in the FishBase life-history tool section. Or we can use these generic estimates which suggest r of 0.6-1.5 for high resilience species (Von Bertalanffy growth rate K>0.3, maturation age < 1, high fecundity) and r of 0.2-1 for medium resilience species:\n\n\n\n\n\nTo learn more about the principles of surplus production models, we recommend this excellent and freely available book by Malcolm Haddon “Using R for Modelling and Quantitative Methods in Fisheries”."
  },
  {
    "objectID": "surplus-production.html#model",
    "href": "surplus-production.html#model",
    "title": "Surplus production models",
    "section": "Model",
    "text": "Model\nBefore applying surplus production models to our stocks it is important to understand how they work and what assumptions they make. To help you explore these assumptions and outcomes, we have developed this user friendly tool which you can access here."
  },
  {
    "objectID": "surplus-production.html#application-of-the-model",
    "href": "surplus-production.html#application-of-the-model",
    "title": "Surplus production models",
    "section": "Application of the model",
    "text": "Application of the model\nSurplus production models have been applied to a wide range of populations and usually perform better than even simpler models that are based only on catch data. For a recent overview of surplus production model applications, please see this publication. Also, when data is limited, simple surplus production models can perform better than more complex models with many parameters, which cannot be effectively estimated, see example publication here.\nHowever, it is important to remember that simple models must make simplifying assumptions. The main assumptions to keep in mind are:\n\nPopulation abundance is mostly controlled by fishing and not other environmental parameters or species interactions. If this is not the case we need other types of models - perhaps models relating stock abundance to prey biomass, temperature, currents or other factors.\nAll spawning stock biomass is equal. We do not care about age or size structure and assume that 1kg of mature 5 year old fish contributes the same amount of production to the stock as 1kg of mature 10 year fish.\nPopulation parameters, such as growth, natural mortality, carrying capacity and regeneration rate are fixed through time and space. Although this is usually not the case, most datasets do not have enough information to estimate changes in these parameters through time anyway."
  },
  {
    "objectID": "temperature_growth.html",
    "href": "temperature_growth.html",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "",
    "text": "Fish growth is strongly dependent on temperature. In warmer conditions fish often growth faster as juveniles, mature at smaller sizes and are smaller as adults. This is known as the temperature size rule, as defined by David Atkinson in 1994.\n\n\n\n\n\nHowever, the mechanisms behind this growth remain unclear. In this model we propose that temperature size rule (TSR) emerges in response to both physiological changes (faster metabolism and intake) and growth and reproduction optimisation to changes in mortality. We develop a physiologically structured life history optimisation model and explore a range of parameters and scenarios under which TSR is likely to emerge. For a brief introduction into the model you can watch these slides or watch this conference talk."
  },
  {
    "objectID": "temperature_growth.html#model-code",
    "href": "temperature_growth.html#model-code",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "Model code",
    "text": "Model code\nThe model is written in Microsoft Excel and R environment. All model code and details are available on this GitHub repository. The main model Excel file can also be downloaded here. In this Excel file you can modify temperature response parameters and their size dependency, optimise the model using Solver option built in Excel and see the resulting growth and reproduction curves, as shown in the figure below."
  },
  {
    "objectID": "temperature_growth.html#application-of-the-model",
    "href": "temperature_growth.html#application-of-the-model",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "Application of the model",
    "text": "Application of the model\nThe open access publication presenting this model is published in a special issue of The Biological Bulletin. If you want to learn more about the role of temperature and oxygen on fish growth, please also check out this overview publication in the same special issue.\nThis model is easy to modify and explore, and you are welcome to use it in your research and teaching. If you would like to suggest or implement new model modifications and publish them, please contact us at lydekaipaliepus@gamtc.lt\n Vendace drawing by Amy Rose Coghlan"
  },
  {
    "objectID": "to_do_list.html",
    "href": "to_do_list.html",
    "title": "TO DO list",
    "section": "",
    "text": "Why is our webiste https://fishsizeproject.github.io/models/ not updated once I push code updates? I can see them locally, but not online"
  },
  {
    "objectID": "to_do_list.html#section",
    "href": "to_do_list.html#section",
    "title": "TO DO list",
    "section": "",
    "text": "Asta: get chlorophyl A image"
  },
  {
    "objectID": "von-bertalanffy.html",
    "href": "von-bertalanffy.html",
    "title": "Models to explore and estiamte von-Bertalanffy growth curves",
    "section": "",
    "text": "One of the most common ways to model fish growth is by using the von Bertalanffy growth curve. The most common version of von Bertalanffy curve describes the length of a fish as a function of three parameters; the asymptotic length \\(L_\\infty\\), the von Bertalanffy growth coefficient \\(k\\) and the theoretical age \\(t0\\) at which fish size is 0.\n\\(L_{(t)} = L_\\infty (1-e^{-k(t-t0)})\\)\nThe \\(k\\) parameter is the instantaneous annual growth rate and it shows how quickly an individual will approach its asymptotic length. In many species \\(1/k\\) gives an approximate age at maturation, although this can vary.\nIn some studies you might also see a two parameter von Bertalanffy curve, which does not include parameter \\(t0\\), but is described as\n\\(L_{(t)} = L_\\infty (1-e^{-kt})\\)\nWhile the two parameter function is commonly used and might be easy to understand, it has been criticised, for example here"
  },
  {
    "objectID": "von-bertalanffy.html#model-code",
    "href": "von-bertalanffy.html#model-code",
    "title": "Models to explore and estiamte von-Bertalanffy growth curves",
    "section": "Model code",
    "text": "Model code\nWe have developed an R shiny application that allows you to explore length growth curves under different von Bertalanffy parameter values. You can use our example age-length data to see how the curve fits the realistic data set (it is already loaded in the app). Or alternatively you can upload your own age-length data, but make sure you keep the same format as our example.\nImportantly, you can explore the histograms of lengths at age, assuming different coefficients of variation (CV) of age-length variability. For example, some populations may have little growth variation, which means that CV is small (CV = 0.1 or smaller), and different age groups are clearly differentiated in their lengths. Other populations might have a lot growth variability (CV = 0.3 or larger), which means that there will be a lot of length overlap among age groups and it will be difficult to separate age classes from length data alone.\nTo start using the model, go here"
  },
  {
    "objectID": "von-bertalanffy.html#application-of-the-model",
    "href": "von-bertalanffy.html#application-of-the-model",
    "title": "Models to explore and estiamte von-Bertalanffy growth curves",
    "section": "Application of the model",
    "text": "Application of the model\nThe von Bertalanffy (VB) growth function is very widely used in fisheries and ecology. You will need the VB growth parameters for most fisheries models and they are also used in the ecosystem models, like this size based ecosystem model for the Curonian lagoon. However, this function should be treated with some caution. To estimate the growth function coefficients reliably we need sufficient amounts of data from young and old individuals. Both might be limited in our data. For example, if a fish stock is heavily fished, we may never see large individuals and will not be able to estimate their asymptotic size. Also, if our data comes from fisheries, rather than independent surveys, we may never see small individuals. If we do not have old fish in the sample the asymptotic length \\(L_\\infty\\) cannot be properly estimated. As a result, estimates of \\(k\\) will also vary extensively across samples.\nFishBase is the largest and most important fish database in the world. In this database you can search for VB growth parameter for your fish species, reported in different studies (scroll all the way down and click on the selection Growth, as in this example for pike). You can see how much these estimates will vary across populations. Part of this variation is due to natural growth differences across populations, but a large fraction is also variation due to uncertain estimates from limited data. For this reason we have developed the application below, so that you can carefully explore how different VB parameter fit your datasets."
  },
  {
    "objectID": "Zero_inflated_model.html",
    "href": "Zero_inflated_model.html",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "",
    "text": "Most fisheries datasets (scientific, commercial, recreational) have a lot of zero catches. These are fishing trips were no fish was caught. These zero catches are important but we need to fit appropriate models to accommodate them. Here we are introducing a model for zero inflated data. This model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling.\nFor a basic introduction into the model and data, check out these slides. However, if you want to use the model we strongly recommend that you watch at least part 4 of our CPUE standardisation course, where the model and approach were presented in more detail.\nBefore proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage."
  },
  {
    "objectID": "Zero_inflated_model.html#model-code",
    "href": "Zero_inflated_model.html#model-code",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "Model code",
    "text": "Model code\nThe main model code is available in this R markdown script and is demonstrated using this dataset. You can download the model and modify the script according to your needs. To look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted catches as a function of fishing time (or other estimate of effort) and month or season, like in the plot below."
  },
  {
    "objectID": "Zero_inflated_model.html#application-of-the-model",
    "href": "Zero_inflated_model.html#application-of-the-model",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "Application of the model",
    "text": "Application of the model\nTo better understand this model and its applications, we strongly recommend that you go through our CPUE standardisation course material, where we discuss different models and their strengths in greater detail. The course also explains how to simulate new datasets using estimated model parameters to assess the probability of obtaining as many zero entries as you have in your dataset (example output of these simulations is shown in the plot below)."
  }
]