[
  {
    "objectID": "Bayesian_inla.html",
    "href": "Bayesian_inla.html",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "",
    "text": "Fisheries catches through time are usually correlated. Catch in one year is not independent of the catch in the previous year. In CPUE standardisation models we should ideally account for this autocorrelation. The GLM based models don’t really do that, despite being a popular approach. Therefore here we will develop and apply a different model which accounts for spatial and temporal correlation in our catches.\nIn this model we will estimate model parameters using approximate Bayesian inference as implemented in the INLA package. The approximate Bayesian inference runs much faster than full inference using MCMC approach and with INLA package it can be applied to a wide range of distributions. For a basic introduction into the Bayesian approach and why it is useful, check out these slides.\nThe slides and the model, together with a detailed description of all the modelling steps have been presented in the last lecture of the CPUE standardisation course, so we recommend you watch the lecture before applying the model. Also, before proceeding with CPUE standardisation, make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage.\nThe model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling."
  },
  {
    "objectID": "Bayesian_inla.html#model-code",
    "href": "Bayesian_inla.html#model-code",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "Model code",
    "text": "Model code\nThe main model code is available in this R markdown script and is demonstrated using this dataset from 22 years of commercial pikeperch (Sander lucioperca) catches in the Baltic Sea. You can download the model and modify the script according to your needs. Before you run the model you will need to install all the R packages, as is explained in this R script.\nTo look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted CPUE and its uncertainty through time, like in the plot below."
  },
  {
    "objectID": "Bayesian_inla.html#application-of-the-model",
    "href": "Bayesian_inla.html#application-of-the-model",
    "title": "Bayesian CPUE standardisation using INLA",
    "section": "Application of the model",
    "text": "Application of the model\nWe are currently working on applying this model to CPUE standardisation of the long term data set from the Curonian Lagoon and Kaunas Water Reservoir in Lithuania. Stay tuned for more outputs. If you are interested to learn more, please contact us as lydekaipaliepus@gamtc.lt"
  },
  {
    "objectID": "curonian_mizer.html",
    "href": "curonian_mizer.html",
    "title": "ENGLISH TITLE MIZER",
    "section": "",
    "text": "ENGLISH TITLE MIZER\nAbout the model (english)\nA link to the code\nA link for more info\nA link to the model\n\n\nLITH TITLE MIZER\nAbout the model (Lithuanian)\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "detecting_size.html",
    "href": "detecting_size.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model (english)\nA link to the code\nA link for more info\nA link to the model\n\n\nAbout the model (Lithuanian)\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "GLM_standardisation.html",
    "href": "GLM_standardisation.html",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "",
    "text": "Before proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared the courses and scripts for data exploration, which you can find on our CPUE standardisation course webpage.\nOne commonly used approach for CPUE data standardisation applies generalized linear models (GLM). Here we model all variables that could impact our catches and extract annual deviations and their uncertainty. From this we can plot a standardized time series of population abundance.\nIn this set of slides you will find main points about GLM based CPUE standardisation.\nFor examples on how this method has been applied to other stocks, see references here and here\n::: {.content-visible when-profile=“english”}"
  },
  {
    "objectID": "GLM_standardisation.html#application-of-the-model",
    "href": "GLM_standardisation.html#application-of-the-model",
    "title": "Standardising catch per unit effort (CPUE) using generalised linear models",
    "section": "Application of the model",
    "text": "Application of the model\nTo learn more about the application of this CPUE standardisation model to Kaunas Water Reservoir fish populations, you can read our publication in journal Fishes.\nYou can also watch this conference talk to learn more about this analysis.\nOnce you conduct your CPUE standardisation, you can use the standardised values in surplus production models, as explained on this website.\n:::"
  },
  {
    "objectID": "growth_change.html",
    "href": "growth_change.html",
    "title": "Detecting changes in fish growth through time",
    "section": "",
    "text": "Although temperature can have large impacts on fish growth, it can be hard to detect it in size and age data, due to inherent noise and variability in growth. To separate temperature signals from other growth variation we need model that will estimate temperature impacts on fish size at age. Here we develop such model and apply it to the long term fish growth observation dataset in Drūkšiai Lake in Lithuania. Drūkšiai lake presents a unique system because during 1994-2010 the lake served as nuclear power plant cooling lake and was heated by nearly 2C. Since 2010 the power plant has been closed and the lake temperature returned to natural levels. Nature Research Centre has been conducting long term monitoring of fish growth has been conducted in Drūkšiai Lake, and now we can use these datasets to estimate how temperature changes affected growth in five fish species."
  },
  {
    "objectID": "growth_change.html#model-code",
    "href": "growth_change.html#model-code",
    "title": "Detecting changes in fish growth through time",
    "section": "Model code",
    "text": "Model code\nThe model is in R environment and is applies Bayesian mixed models of length as a function of age, age squared, lifetime average temperature and the interaction between lifetime average temperature and age. The model code and output can be found here. The model estimates the intercept and slope of temperature effect on size at age. In this way we assess whether all ages growth faster or slower (sign and magnitude of the intercept) and whether temperature impacts are different for young versus older ages (sign and magnitude of the slope). You can see model predictions in the figure below."
  },
  {
    "objectID": "growth_change.html#application-of-the-model",
    "href": "growth_change.html#application-of-the-model",
    "title": "Detecting changes in fish growth through time",
    "section": "Application of the model",
    "text": "Application of the model\nThis model and its results have been presented at the ICES annual conference as an in September 2022 - see video here.\nIt also was presented at the 151st American Fisheries Society Annual Meeting in November 2021 - see video here\nIf you would like to suggest or implement new model modifications and publish them, please contact us at lydekaipaliepus@gamtc.lt\nPerch drawing by Amy Rose Coghlan"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models for sustainable inland fisheries",
    "section": "",
    "text": "This is a webpage of models developed during the project “Advanced models, citizen science and big data for sustainable food production and ecological services of inland aquatic ecosystems”. The project has received funding from European Regional Development Fund (project No 01.2.2-LMT-K-718-02-0006) under grant agreement with the Research Council of Lithuania (LMTLT).\nThis page and models presented here have been created by Asta Audzijonyte, Carl Smith, Catarina Silva, Egle Jakubavičiūtė, Dalia Grendaitė, Max Lindmark, Freddie Heather, Steve Midway and Gustav Delius.\nIf you have questions and suggestions please contact us at lydekaipaliepus@gamtc.lt\nStart exploring models\nGo back to the project website."
  },
  {
    "objectID": "jabba_models.html",
    "href": "jabba_models.html",
    "title": "Surplus production models",
    "section": "",
    "text": "JABBA is a Bayesian State-Space Surplus Production Model framework developed by Winker et al. 2018.\nIt uses catch and relative abundance time series and requires prior information on a) the resilience parameter r (intrinsic rate of population increase), b) carrying capacity K and c) the relative initial biomass at the beginning of the time series.\nYou can find all information in the developers’ [vignette]"
  },
  {
    "objectID": "jabba_models.html#model-code",
    "href": "jabba_models.html#model-code",
    "title": "Surplus production models",
    "section": "Model code",
    "text": "Model code\nHere is an example code of how we used JABBA for modeling five species in the Curonian Lagoon and Kaunas water reservoir (Lithuania)."
  },
  {
    "objectID": "jabba_models.html#application-of-the-model",
    "href": "jabba_models.html#application-of-the-model",
    "title": "Surplus production models",
    "section": "Application of the model",
    "text": "Application of the model\nTo learn more about surplus production modeling to Curonian Lagoon and Kaunas Water Reservoir fish stocks and results that could be of interest to managers, you can read this and this summary or watch this and this conference talks."
  },
  {
    "objectID": "ML_fish_size.html",
    "href": "ML_fish_size.html",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "",
    "text": "Here we present a simple approach to develop models for estimating fish size classes from images. We have prepared a scripts for data pre-processing and machine learning model development, which you can find on our Online course webpage.\nIn this publication you will find a detailed description of the method and a pilot case-study where we demonstrate potential use for estimating fish size classes from images without a speficied reference object. You can also find all the scripts used in the framework in our Github page."
  },
  {
    "objectID": "ML_fish_size.html#model-code",
    "href": "ML_fish_size.html#model-code",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "Model code",
    "text": "Model code\nTo use our models it is very important that fish size classes in your dataset are identified correctly, otherwise your model will not be very useful. You can upload images (in JPEG or PNG format) to your Google drive by size class (i.e. fish images of each size class per folder), following this directory structure:\ndataset\n|__ class5\n    |______ 100080576_f52e8ee070_n.PNG\n    |______ 14167534527_781ceb1b7a_n.PNG\n    |______ ...\n|__ class10\n    |______ 10043234166_e6dd915111_n.PNG\n    |______ 1426682852_e62169221f_m.PNG\n    |______ ...\n|__ class15\n    |______ 102501987_3cdb8e5394_n.PNG\n    |______ 14982802401_a3dfb22afb.PNG\n    |______ ...\n|__ ...\n\nTo apply our model you can use this code:\n\n\n\nOpen In Colab"
  },
  {
    "objectID": "ML_fish_size.html#application-of-the-model",
    "href": "ML_fish_size.html#application-of-the-model",
    "title": "Estimating fish size classes using image classification and machine learning models",
    "section": "Application of the model",
    "text": "Application of the model\nYou can also watch this talk for the “Online seminar: Recreational angling effort and engagement in the digital age” to learn more about this model."
  },
  {
    "objectID": "ML_fish_species.html",
    "href": "ML_fish_species.html",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "",
    "text": "Here we present a scalable user-friendly artificial intelligence (AI) framework to develop fish species identification models. We have prepared a course and scripts for data pre-processing, image processing and machine learning model development, which you can find on our Online course webpage.\nIn this publication in the journal Sustainability you will find a detailed description of the framework, a pilot case-study where we demonstrate potential use for recreational fisheries research, a summary of the knowledge gained from the case study application and an outline of the main challenges and potential future development. You can also find all the scripts used in the framework in our Github page.\nOpen-source modular framework for large scale image storage, handling, annotation and automatic classification"
  },
  {
    "objectID": "ML_fish_species.html#model-code",
    "href": "ML_fish_species.html#model-code",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "Model code",
    "text": "Model code\nTo use our models it is very important that fish species (or ecotypes or any other classes you want your model to identify) in your dataset are identified correctly, otherwise your model will not be very useful (you might have heard the expression “garbage in - garbage out”). You can upload images (in JPEG or PNG format) to your Google drive by species (i.e. one species per folder - please make sure these are correctly identified), following this directory structure:\ndataset\n|__ perch\n    |______ 100080576_f52e8ee070_n.PNG\n    |______ 14167534527_781ceb1b7a_n.PNG\n    |______ ...\n|__ striped_bass\n    |______ 10043234166_e6dd915111_n.PNG\n    |______ 1426682852_e62169221f_m.PNG\n    |______ ...\n|__ trout\n    |______ 102501987_3cdb8e5394_n.PNG\n    |______ 14982802401_a3dfb22afb.PNG\n    |______ ...\n|__ ...\n\nTo apply our model you can use this code:"
  },
  {
    "objectID": "ML_fish_species.html#application-of-the-model",
    "href": "ML_fish_species.html#application-of-the-model",
    "title": "Machine learning based image collection, annotation and classification of fish species",
    "section": "Application of the model",
    "text": "Application of the model\nYou can also watch this talk for the “Online workshop: Machine learning, fishing and citizen science” and this talk for the “Online seminar: Recreational angling effort and engagement in the digital age” to learn more about this model."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Overview of models",
    "section": "",
    "text": "We have developed and applied a range of statistical and mechanistic models, which we used in our studies and are happy to share with others. We hope these models will be useful for fisheries research and management. The models are available for free non-commercial use, but please reference the original source (publications or this website).\n\n\nScientific monitoring and artisanal, commercial or recreational fish catch data is often used to assess population status and trends, but such data are usually complex and require careful standardisation. There are several approaches that can be used for CPUE standardisation and we present three potential models of increasing complexity."
  },
  {
    "objectID": "models.html#surplus-production-models-for-stock-assessments",
    "href": "models.html#surplus-production-models-for-stock-assessments",
    "title": "Overview of models",
    "section": "Surplus production models for stock assessments",
    "text": "Surplus production models for stock assessments\nSurplus production (SP) models are commonly used to assess data-poor fish stocks and are based on time series of catches and population abundance index (such as standardised CPUE time series, from the models above). These models assume that fish population abundance depends on its regeneration rate, carrying capacity and catches. SP models have been successfully applied to many stocks and, despite their simple assumptions, often perform surprisingly well, assuming the population abundance and catch time series are reliable."
  },
  {
    "objectID": "models.html#fish-growth-models",
    "href": "models.html#fish-growth-models",
    "title": "Overview of models",
    "section": "Fish growth models",
    "text": "Fish growth models\nFish growth and therefore body sizes are highly variable, as it depends on temperature, food availability and many other processes. Understanding and modelling how and why growth can change is therefore an important part of fisheries research."
  },
  {
    "objectID": "models.html#machine-learning-models",
    "href": "models.html#machine-learning-models",
    "title": "Overview of models",
    "section": "Machine learning models",
    "text": "Machine learning models\nMachine learning (ML) enables rapid analyses of large image and datasets and is an important step to facilitate citizen science driven data collection techniques. During our project we have developed two machine learning models for fish species and size identification."
  },
  {
    "objectID": "models.html#satellite-data-analysis",
    "href": "models.html#satellite-data-analysis",
    "title": "Overview of models",
    "section": "Satellite data analysis",
    "text": "Satellite data analysis\nRemote sensing and satellite data is now being collected on high temporal and spatial resolution. However, it is often challenging to access and process these data. We have developed some models and tools to aid with satellite based water surface temperature and chlorophyll A data acquisition and analysis."
  },
  {
    "objectID": "models.html#size-based-ecosystem-models",
    "href": "models.html#size-based-ecosystem-models",
    "title": "Overview of models",
    "section": "Size based ecosystem models",
    "text": "Size based ecosystem models\nSize based community and ecosystem models represent a powerful tool to explore potential outcomes of different fisheries management strategies, species interactions, climate change and a lot more. In this project we are developing a size based model for the Curonian lagoon ecosystem.\n\n\n\n\n\n\n\n14. Sized based models for Curonian lagoon and Baltic Sea  Learn about the potential climate change impacts in the Baltic Sea, explore a basic Curonian lagoon model and understand key principles of size based modelling, as implemented in a R package mizer."
  },
  {
    "objectID": "mqmf.html",
    "href": "mqmf.html",
    "title": "MQMF",
    "section": "",
    "text": "About the model\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "satelite_chla.html",
    "href": "satelite_chla.html",
    "title": "MQMF",
    "section": "",
    "text": "ENGLISH TITLE\nAbout the model (english)\nA link to the code\nA link for more info\nA link to the model\n\n\nLITH TITLE\nAbout the model (Lithuanian)\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "satelite_temp.html",
    "href": "satelite_temp.html",
    "title": "ENGLISH TITLE",
    "section": "",
    "text": "ENGLISH TITLE\nAbout the model (english)\nA link to the code\nA link for more info\nA link to the model\n\n\nLITH TITLE\nAbout the model (Lithuanian)\nA link to the code\nA link for more info\nA link to the model"
  },
  {
    "objectID": "scripts/BayesianINLA.html",
    "href": "scripts/BayesianINLA.html",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "",
    "text": "Note that we are using a data file that has already been explored and modified to remove outliers. Read the introduction to the model to see where to find the original data file and explanation on data exploration"
  },
  {
    "objectID": "scripts/BayesianINLA.html#data-exploration",
    "href": "scripts/BayesianINLA.html#data-exploration",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Data exploration",
    "text": "Data exploration\n\nCheck outliers\nThe plots below suggest that there are no outliers in the dataset\n\n\n\n\n\n\n\nNormality and homogeneity of variance\nThe plots below show that as catches and effort increase, so does the variance. This suggest a departure from the homogeneity of variance."
  },
  {
    "objectID": "scripts/BayesianINLA.html#fit-increasingly-complex-models",
    "href": "scripts/BayesianINLA.html#fit-increasingly-complex-models",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Fit increasingly complex models",
    "text": "Fit increasingly complex models\nWe start with an intercept only model. Catch is modelled as a function of year, whereas ‘rw1’ part imposes a temporal trend. We fit a gamma distribution using INLA. A gamma distribution is strictly positive (no zeros) and skewed (like our catch data)\n\n# Create a formula\nf1 <- Catch ~ + f(Year, model = \"rw1\")\n\n#fit a model\nI1 <- inla(f1, \n           control.compute = list(dic = TRUE), #estimate dic for model comparison\n           family = \"Gamma\",\n           data = zan)\n\n# Plot the time(year) trend\nYearsm <- I1$summary.random$Year\nFit1   <- I1$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,1), mar = c(5,5,2,2), cex.lab = 1.5)\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1))\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\n\n\n\nThe result above shows that if we only look at one trend, we see decreasing catches through time. But effort also might have changed, so we need to include effort in the model.\n\n#create a formula with effort\nf2 <- Catch ~ Effort + f(Year, model = \"rw1\")\n\n#fit a model\nI2 <- inla(f2, \n           control.compute = list(dic = TRUE),\n           family = \"Gamma\",\n           data = zan)\n#compare two models using a criterion similar to AIC\nround(I1$dic$dic,0) #4179\n\n[1] 4179\n\nround(I2$dic$dic,0) #3532 <- including Effort improves fit\n\n[1] 3532\n\n#And plot the trend \nYearsm <- I2$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-0.2, 0.2) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\n\n\n\nNow the trend is clearly different. This is because we are not including effort in the model, which is important. However, we also need to account for potentially different trends across months. So now we will nest month within a year\n\n#create a formula\nf3 <- Catch ~ Effort + \n  f(Year, \n    model = \"rw1\") +\n  f(Mon, \n    model = \"rw1\", cyclic = TRUE)  \n\n#fit the model\nI3 <- inla(f3, \n           control.predictor = list(compute = TRUE),\n           control.compute = list(dic = TRUE),\n           family = \"gamma\",\n           data = zan)\n\n#compare with the previous model \nround(I2$dic$dic,0) #3525\n\n[1] 3532\n\nround(I3$dic$dic,0) #3203 <- including month improves fit\n\n[1] 3203\n\n#plot the trend, but this time we plot it for years and months\nFit3 <- I3$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)\nYearsm   <- I3$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\nMonsm <- I3$summary.random$Mon\nplot(Monsm[,1:2], type='l',\n     xlab = 'MonthInYear', \n     ylab = '',\n     ylim = c(-4, 4) )\nabline(h=0, lty=3)\nlines(Monsm[, c(1, 4)], lty=2)\nlines(Monsm[, c(1, 6)], lty=2)\n\n\n\n\nWe can see that catches change a lot during the year and are low in summer months. So we definitely need to include month in the model. Since we have several stations, we probably also need to include stations in the model.\n\n# create a formula with station as a random term\nf4 <- Catch ~ Effort + \n  f(Year, \n    model = \"rw1\") +\n  f(Mon, \n    model = \"rw1\", cyclic = TRUE) +\n  f(fStn, model = \"iid\") \n\n#fit the model \nI4 <- inla(f4, \n           control.predictor = list(compute = TRUE),\n           control.compute = list(dic = TRUE),\n           family = \"Gamma\",\n           data = zan)\n\n#compare with the previous model \nround(I3$dic$dic,0) #3203\n\n[1] 3203\n\nround(I4$dic$dic,0) #2977 <- including station improves fit\n\n[1] 2977\n\n#Plot trend across years and months \nFit4 <- I4$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)\nYearsm   <- I4$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\nMonsm   <- I4$summary.random$Mon\nplot(Monsm[,1:2], type='l',\n     xlab = 'MonthInYear', \n     ylab = 'Random walk trend',\n     ylim = c(-3, 3) )\nabline(h=0, lty=3)\nlines(Monsm[, c(1, 4)], lty=2)\nlines(Monsm[, c(1, 6)], lty=2)"
  },
  {
    "objectID": "scripts/BayesianINLA.html#explore-the-best-model",
    "href": "scripts/BayesianINLA.html#explore-the-best-model",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Explore the best model",
    "text": "Explore the best model\nNow we look at the Bayesian residuals\n\n\n\n\n\n\n\n\n\n\n\nThese residuals look ok. However, if we fit residuals versus effort we see a strange non linear pattern, which suggests that the model does not fully fit. We will then plot it for each station and find that there seem to be different residual and therefore likely also CPUE trends in different stations.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nTo solve this problem we make station a fixed effect and add an interaction with effort to capture non-linearity. If different stations have different catch trends this interaction should be included. So we define a yet another model with Effort * fStn interaction and fit it.\n\n#define the model\nf5 <- Catch ~ Effort * fStn + \n  f(Year, \n    model = \"rw1\") +\n  f(Mon, \n    model = \"rw1\",\n        cyclic = T)\n#fit the model \nI5 <- inla(f5, \n           control.predictor = list(compute = TRUE),\n           control.compute = list(config = TRUE, dic = TRUE),\n           family = \"Gamma\",\n           control.family = list(link = \"log\"),\n           data = zan)\n\n#compare wit the previous model \nround(I4$dic$dic,0) #2977 \n\n[1] 2977\n\nround(I5$dic$dic,0) #2889 <- big improvement in fit...\n\n[1] 2889\n\n#plot trends \nFit5 <- I5$summary.fitted.values[,\"mean\"]\n\npar(mfrow = c(1,2), mar = c(5,5,2,2), cex.lab = 1.5)\nYearsm   <- I5$summary.random$Year\nplot(Yearsm[,1:2], type='l',\n     xlab = 'Year', \n     ylab = 'Random walk trend',\n     ylim = c(-1, 1) )\nabline(h=0, lty=3)\nlines(Yearsm[, c(1, 4)], lty=2)\nlines(Yearsm[, c(1, 6)], lty=2)\n\nMonsm   <- I5$summary.random$Mon\nplot(Monsm[,1:2], type='l',\n     xlab = 'MonthInYear', \n     ylab = 'Random walk trend',\n     ylim = c(-3, 3) )\nabline(h=0, lty=3)\nlines(Monsm[, c(1, 4)], lty=2)\nlines(Monsm[, c(1, 6)], lty=2)\n\n\n\n\nNow we look at the residuals of our new model\n\n# Get the fitted values and Pearson residuals\nN     <- nrow(zan)\nmu2   <- I5$summary.fitted.values[1:N,\"mean\"] \nr     <- I5$summary.hyperpar[\"Precision parameter for the Gamma observations\", \"mean\"]\nVarY2 <- mu2^2 / r\nE2    <- (zan$Catch - mu2) / sqrt(VarY2)\n\n# Plot residuals versus fitted values.\npar(mfrow = c(1, 1))\nplot(x = mu2, \n     y = E2,\n     xlab = \"Fitted values\",\n     ylab = \"Pearson residuals\")\nabline(h = 0, lty = 2, col = 1)\n\n\n\n# Plot residuals versus station\nboxplot(E2 ~ Station, \n        ylab = \"Pearson residuals\",\n        data = zan)\nabline(h = 0, lty = 2)\n\n\n\n# Year\nboxplot(E2 ~ Year, \n        ylab = \"Pearson residuals\",\n        data = zan)\nabline(h = 0, lty = 2)\n\n\n\n# Month\nboxplot(E2 ~ Month, \n        ylab = \"Pearson residuals\",\n        data = zan)\nabline(h = 0, lty = 2)\n\n\n\n# Residuals versus effort\nzan$E2 <- E2\n\nresplot2 <- ggplot() +\n  geom_point(data = zan, alpha = 0.4, size = 2,\n             aes(y = E2 ,  \n                 x = Effort)) +\n  geom_smooth(data = zan,                    \n              aes(y = E2, \n                  x = Effort)) +\n  xlab(\"Effort\") + ylab(\"Pearson residuals\") +\n  theme(text = element_text(size = 12), legend.position=\"none\") +\n  theme(axis.text.x = element_text(size = 11, angle = 45, hjust = 0.9)) +\n  My_theme +\n  geom_hline(yintercept = 0, col = 2)\nresplot2\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe final plot shows that the significant nonlinear pattern in residuals is mostly gone (uncertainty ranges include zero)."
  },
  {
    "objectID": "scripts/BayesianINLA.html#plot-model-outputs",
    "href": "scripts/BayesianINLA.html#plot-model-outputs",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Plot model outputs",
    "text": "Plot model outputs\nFirst we plots the overall temporal trend in years and months\n\n# Plot temporal effects\np1 <- bind_rows(\n  I5$summary.random$Year %>%\n    select(Year = 1, mean = 2, lcl = 4, ucl = 6) %>%\n    mutate(Model = \"rw1\")\n) %>%\n  ggplot(aes(x = Year, y = mean, ymin = lcl, ymax = ucl)) +\n  geom_ribbon(alpha = 0.2, fill = \"forestgreen\") +\n  geom_line(colour = \"forestgreen\") + My_theme +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n             color = \"firebrick2\", size=0.4) +\n  ggtitle(\"Year\") +\n  theme(legend.position = \"none\")\nggplotly(p1)\n\n\n\n\np2 <- bind_rows(\n  I5$summary.random$Mon %>%\n    select(Mon = 1, mean = 2, lcl = 4, ucl = 6) %>%\n    mutate(Model = \"rw2\")\n) %>%\n  ggplot(aes(x = Mon, y = mean, ymin = lcl, ymax = ucl)) +\n  geom_ribbon(alpha = 0.2, fill = \"dodgerblue2\") +\n  geom_line(colour = \"dodgerblue2\") + My_theme +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n             color = \"firebrick2\", size=0.4) +\n  ggtitle(\"Month within Year\") +\n  theme(legend.position = \"none\")\nggplotly(p2)"
  },
  {
    "objectID": "scripts/BayesianINLA.html#plot-catch-versus-effort",
    "href": "scripts/BayesianINLA.html#plot-catch-versus-effort",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Plot catch versus effort",
    "text": "Plot catch versus effort\nHere we plot the model output where we show catch as a function of effort for each station. Note that these predictions will be shown for one selected year (year 2002 in this case) and month (September), whereas data scatterplot shows the full data."
  },
  {
    "objectID": "scripts/BayesianINLA.html#plot-cpue-through-time",
    "href": "scripts/BayesianINLA.html#plot-cpue-through-time",
    "title": "R code for Bayesian CPUE time series standardisation using INLA",
    "section": "Plot CPUE through time",
    "text": "Plot CPUE through time\nNow we plot CPUE through time for a selected station (station 1) and selected effort level (20K), for all months"
  },
  {
    "objectID": "scripts/CPUEstand_example.html",
    "href": "scripts/CPUEstand_example.html",
    "title": "CPUE standartisation using GLM_Example StockC",
    "section": "",
    "text": "load libraries\n###import and explore dataset\n\n#import dataset\nstockC <- read.csv(file = 'stockC2.csv', fileEncoding=\"UTF-8-BOM\") \n\n#check how it looks\nhead(stockC)\n\n  Fishing_id Location Year season Gillnet_category Net.mesh.gr Net_length_Or\n1          1   Middle 2014 spring           capron       small             1\n2          1   Middle 2014 spring           capron       small             1\n3          1   Middle 2014 spring           capron       small             1\n4          1   Middle 2014 spring           capron       small             1\n5          1   Middle 2014 spring           capron       small             1\n6          1   Middle 2014 spring           capron       small             1\n  Soak_time_Or Catch_g\n1            1       0\n2            1       0\n3            1      96\n4            1       0\n5            1       0\n6            1       0\n\nstr(stockC)\n\n'data.frame':   2296 obs. of  9 variables:\n $ Fishing_id      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Location        : chr  \"Middle\" \"Middle\" \"Middle\" \"Middle\" ...\n $ Year            : int  2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ...\n $ season          : chr  \"spring\" \"spring\" \"spring\" \"spring\" ...\n $ Gillnet_category: chr  \"capron\" \"capron\" \"capron\" \"capron\" ...\n $ Net.mesh.gr     : chr  \"small\" \"small\" \"small\" \"small\" ...\n $ Net_length_Or   : int  1 1 1 1 1 1 2 2 2 2 ...\n $ Soak_time_Or    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Catch_g         : num  0 0 96 0 0 0 0 0 0 0 ...\n\n#in this dataset, net length and soak time are indicated as ordered factors, so make sure they are treated like that. Also, Year must be a factor\n\nstockC$Year<-as.factor(stockC$Year)\nstockC$Soak_time_Or<-as.ordered(stockC$Soak_time_Or)\nstockC$Net_length_Or<-as.ordered(stockC$Net_length_Or)\nstockC$season <- as.factor(stockC$season)\n\n#which seasons do we have\nunique(stockC$season)\n\n[1] spring summer fall  \nLevels: fall spring summer\n\n#balance among seasons \ntable(stockC$season)\n\n\n  fall spring summer \n   351    833   1112 \n\n#how many locations\nunique(stockC$Location)\n\n[1] \"Middle\"     \"Upstream\"   \"Downstream\" NA          \n\n#balance among locations\ntable(stockC$Location)\n\n\nDownstream     Middle   Upstream \n        26       1233       1034 \n\n#gillnet categories and balance \nunique(stockC$Gillnet_category)\n\n[1] \"capron\" \"nylon\"  \"other\" \n\ntable(stockC$Gillnet_category)\n\n\ncapron  nylon  other \n  1079   1210      7 \n\n#probably we should remove category \"other\" in gillnets\nstockC<-stockC %>% filter (Gillnet_category!=\"other\")\n\n###Build models Start GLM, with distribution family tweedie, which allows many zeros in the distribution.\n\n#build model with all factors \n\nmodC1<- glm(Catch_g~Year + season + Location + Gillnet_category + Net.mesh.gr + Net_length_Or +Soak_time_Or, family=tweedie(var.power=1.1, link.power=0), data=stockC)\nsummary(modC1)\n\n\nCall:\nglm(formula = Catch_g ~ Year + season + Location + Gillnet_category + \n    Net.mesh.gr + Net_length_Or + Soak_time_Or, family = tweedie(var.power = 1.1, \n    link.power = 0), data = stockC)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-190.729   -31.237    -8.590    -3.532   264.681  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            6.56638    2.15233   3.051 0.002309 ** \nYear1992              -2.02409    0.21846  -9.265  < 2e-16 ***\nYear1993              -1.34608    0.19184  -7.017 3.00e-12 ***\nYear1994              -2.27349    0.29266  -7.768 1.20e-14 ***\nYear1995              -1.62397    0.19564  -8.301  < 2e-16 ***\nYear1996              -0.56539    0.26576  -2.127 0.033496 *  \nYear1997              -1.30138    0.27689  -4.700 2.76e-06 ***\nYear1998              -1.47752    0.60221  -2.453 0.014224 *  \nYear1999              -2.64934    0.26765  -9.898  < 2e-16 ***\nYear2000              -2.44237    0.30916  -7.900 4.33e-15 ***\nYear2001              -4.33690    1.19796  -3.620 0.000301 ***\nYear2002              -2.55302    0.29924  -8.532  < 2e-16 ***\nYear2003              -1.75940    0.24607  -7.150 1.17e-12 ***\nYear2004              -3.25933    0.39188  -8.317  < 2e-16 ***\nYear2005              -3.15205    0.44637  -7.061 2.19e-12 ***\nYear2006              -5.06887    1.94041  -2.612 0.009054 ** \nYear2007              -4.13477    0.69318  -5.965 2.84e-09 ***\nYear2008              -3.49185    0.39847  -8.763  < 2e-16 ***\nYear2009              -6.54723    2.59571  -2.522 0.011727 *  \nYear2010              -3.27110    0.85260  -3.837 0.000128 ***\nYear2011              -5.91710    1.93087  -3.064 0.002206 ** \nYear2012              -4.35357    0.55442  -7.852 6.27e-15 ***\nYear2013              -4.13633    0.47565  -8.696  < 2e-16 ***\nYear2014              -4.52713    0.28749 -15.747  < 2e-16 ***\nYear2015              -4.06171    0.32880 -12.353  < 2e-16 ***\nYear2016              -3.17572    0.28116 -11.295  < 2e-16 ***\nYear2017              -4.01908    0.24690 -16.278  < 2e-16 ***\nYear2018              -3.34323    0.25499 -13.111  < 2e-16 ***\nYear2019              -3.96988    0.30720 -12.923  < 2e-16 ***\nYear2020              -3.17021    0.27249 -11.634  < 2e-16 ***\nYear2021              -3.20703    0.28225 -11.362  < 2e-16 ***\nseasonspring           0.45110    0.16231   2.779 0.005494 ** \nseasonsummer          -0.13658    0.13214  -1.034 0.301421    \nLocationMiddle         3.47547    2.14144   1.623 0.104738    \nLocationUpstream       3.25628    2.13955   1.522 0.128165    \nGillnet_categorynylon  0.30745    0.12268   2.506 0.012277 *  \nNet.mesh.grfull       -2.17180    0.59549  -3.647 0.000271 ***\nNet.mesh.grsmall      -2.09590    0.37860  -5.536 3.46e-08 ***\nNet_length_Or.L        1.02684    0.45385   2.263 0.023761 *  \nNet_length_Or.Q       -1.23429    0.36691  -3.364 0.000781 ***\nNet_length_Or.C        0.05849    0.21704   0.269 0.787575    \nNet_length_Or^4       -0.02982    0.11461  -0.260 0.794718    \nSoak_time_Or.L        -0.02503    0.09397  -0.266 0.790008    \nSoak_time_Or.Q         0.40939    0.29936   1.368 0.171584    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 2626.631)\n\n    Null deviance: 8267841  on 2286  degrees of freedom\nResidual deviance: 3412680  on 2243  degrees of freedom\n  (2 observations deleted due to missingness)\nAIC: NA\n\nNumber of Fisher Scoring iterations: 9\n\n#Can we drop anything? I.e. is model without certain variable significantly different from the one that includes it?\ndrop1(modC1, test = \"F\")\n\nSingle term deletions\n\nModel:\nCatch_g ~ Year + season + Location + Gillnet_category + Net.mesh.gr + \n    Net_length_Or + Soak_time_Or\n                 Df Deviance F value    Pr(>F)    \n<none>               3412680                      \nYear             30  4877582 32.0938 < 2.2e-16 ***\nseason            2  3463962 16.8525 5.439e-08 ***\nLocation          2  3437273  8.0818 0.0003182 ***\nGillnet_category  1  3429358 10.9612 0.0009451 ***\nNet.mesh.gr       2  3603231 62.6202 < 2.2e-16 ***\nNet_length_Or     4  3501726 14.6314 8.403e-12 ***\nSoak_time_Or      2  3418146  1.7963 0.1661473    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#yes, soak time is not signif\n\nAICtweedie(modC1)  \n\n[1] 21002.13\n\nmodEvA::Dsquared(modC1) *100  # % of explained deviance\n\n[1] 58.72344\n\n#Soak time is not signif, so try without it and then compare the models \n\nmodC2<- glm(Catch_g~Year + season + Location + Gillnet_category + Net.mesh.gr + Net_length_Or,  family=tweedie(var.power=1.1, link.power=0), data=stockC)\nsummary(modC2)\n\n\nCall:\nglm(formula = Catch_g ~ Year + season + Location + Gillnet_category + \n    Net.mesh.gr + Net_length_Or, family = tweedie(var.power = 1.1, \n    link.power = 0), data = stockC)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-192.061   -31.289    -8.716    -3.506   267.393  \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            6.69431    2.15070   3.113 0.001878 ** \nYear1992              -2.02577    0.21849  -9.272  < 2e-16 ***\nYear1993              -1.37024    0.19123  -7.166 1.05e-12 ***\nYear1994              -2.27573    0.29231  -7.785 1.05e-14 ***\nYear1995              -1.61607    0.19560  -8.262 2.41e-16 ***\nYear1996              -0.56401    0.26436  -2.134 0.032992 *  \nYear1997              -1.28799    0.27685  -4.652 3.47e-06 ***\nYear1998              -1.99196    0.47597  -4.185 2.96e-05 ***\nYear1999              -2.68313    0.26119 -10.273  < 2e-16 ***\nYear2000              -2.45445    0.30445  -8.062 1.21e-15 ***\nYear2001              -4.34204    1.19921  -3.621 0.000300 ***\nYear2002              -2.55788    0.29420  -8.694  < 2e-16 ***\nYear2003              -1.76295    0.24441  -7.213 7.45e-13 ***\nYear2004              -3.24925    0.39127  -8.304  < 2e-16 ***\nYear2005              -3.15464    0.44574  -7.077 1.96e-12 ***\nYear2006              -5.07667    1.94157  -2.615 0.008990 ** \nYear2007              -4.15134    0.68927  -6.023 2.00e-09 ***\nYear2008              -3.49539    0.39845  -8.773  < 2e-16 ***\nYear2009              -6.54048    2.59931  -2.516 0.011931 *  \nYear2010              -3.27361    0.85337  -3.836 0.000128 ***\nYear2011              -5.91998    1.93276  -3.063 0.002218 ** \nYear2012              -4.34787    0.55478  -7.837 7.06e-15 ***\nYear2013              -4.11711    0.47347  -8.696  < 2e-16 ***\nYear2014              -4.51562    0.28500 -15.844  < 2e-16 ***\nYear2015              -4.05112    0.32753 -12.369  < 2e-16 ***\nYear2016              -3.18036    0.28071 -11.330  < 2e-16 ***\nYear2017              -4.01082    0.24590 -16.311  < 2e-16 ***\nYear2018              -3.33918    0.25502 -13.094  < 2e-16 ***\nYear2019              -3.96459    0.30721 -12.905  < 2e-16 ***\nYear2020              -3.16691    0.27283 -11.608  < 2e-16 ***\nYear2021              -3.20159    0.28220 -11.345  < 2e-16 ***\nseasonspring           0.47465    0.12725   3.730 0.000196 ***\nseasonsummer          -0.13181    0.13109  -1.005 0.314794    \nLocationMiddle         3.46678    2.14319   1.618 0.105894    \nLocationUpstream       3.25472    2.14140   1.520 0.128675    \nGillnet_categorynylon  0.30602    0.12279   2.492 0.012767 *  \nNet.mesh.grfull       -2.16351    0.59578  -3.631 0.000288 ***\nNet.mesh.grsmall      -2.11404    0.37812  -5.591 2.53e-08 ***\nNet_length_Or.L        0.91549    0.44917   2.038 0.041648 *  \nNet_length_Or.Q       -1.30913    0.36465  -3.590 0.000338 ***\nNet_length_Or.C        0.01269    0.21551   0.059 0.953062    \nNet_length_Or^4       -0.03656    0.11455  -0.319 0.749649    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Tweedie family taken to be 2631.836)\n\n    Null deviance: 8267841  on 2286  degrees of freedom\nResidual deviance: 3418146  on 2245  degrees of freedom\n  (2 observations deleted due to missingness)\nAIC: NA\n\nNumber of Fisher Scoring iterations: 9\n\nAICtweedie(modC2) \n\n[1] 21008.01\n\nmodEvA::Dsquared(modC2) *100  # % of explained deviance\n\n[1] 58.65733\n\nAICtweedie(modC2)- AICtweedie(modC1)\n\n[1] 5.884925\n\n#AIC is the lower for modC1 so consider it the best model\n\n#Extract coefficients and plot standardised CPUE\n\n##extract coefficients. We can use a little function scaleCE that will scale the cpue against the first Year \ntt <- summary(modC1)$coefficients  # combine these with empty first Year\ntt2 <- rbind(c(0,0,0,0), tt[grep(\"Year\",rownames(tt)),])\ntt3 <- cbind(tt2, exp(tt2[,\"Estimate\"]), scaleCE(exp(tt2[,\"Estimate\"])))\nyrs<- c(1991:2021)\ncpue_stand <- cbind(tt3, yrs)\ncolnames(cpue_stand) <- c( \"Estimate\", \"StdError\", \"t value\" ,\"Pr(>|t|)\" , \"Exp_estimate\" ,\"Scaled_estimate\", \"Year\")\nrm(tt, tt2, tt3)\n\ncpue_stand <- as.data.frame(cpue_stand)\n\n#Plot - need to start the Year before the dataset as it uses that to scale to\nplot(cpue_stand$Year, cpue_stand$Scaled_estimate, ylab=\"Standardised CPUE\", main=\"Stock C\", xlab=\"Year\",pch= 19)\n\n\n\n#ggplot\nggplot(cpue_stand, aes(Year, Scaled_estimate)) +      \n  geom_point(data = cpue_stand, aes(x = Year, y = Scaled_estimate), size=2) +\n  #geom_errorbar(data = cpue_stand, aes(ymin = Scaled_estimate, ymax = Scaled_estimate+StdError)) +\n    ylab(\"Standardised CPUE\") +\n  xlab(\"\") +\n   ggtitle(\"\") +\n  geom_smooth(method = \"loess\") +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\npanel.background = element_blank(), axis.line = element_line(colour = \"black\")) +\n  theme(axis.text=element_text(size=14),\n        axis.title=element_text(size=14))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n#save\n#write.csv(cpue_stand, \"cpue_standStockC1.csv\")\n\n#Diagnostics\n\n#plot model coefficients\nplot_model(modC1)\n\n\n\n#plot effects\nplot(allEffects(modC1))\n\n\n\n#Check residuals and outliers, try again without them if needed\n#model diagnostics\nautoplot(modC1, which = 1:6, label.size = 3)+ \n  theme_bw()\n\n\n\n#outliers- quantitative detection\n\n#leverage  distance\ni_n = influence(modC1)$hat # calculate the influence of data points\nwhich.max(i_n)   #shows which data point has the highest influence on the fitted model\n\n1658 \n1658 \n\nstockC[1658,]\n\n     Fishing_id Location Year season Gillnet_category Net.mesh.gr Net_length_Or\n1658        208 Upstream 1998 spring            nylon         big             3\n     Soak_time_Or Catch_g\n1658            2   21072\n\n#Cook’s distance.\nc_d = cooks.distance(modC1)\nwhich.max(c_d)  ##shows which data point has the highest influence on the fitted model\n\n1699 \n1699 \n\nstockC[1699,]\n\n     Fishing_id Location Year season Gillnet_category Net.mesh.gr Net_length_Or\n1699        244 Upstream 1996   fall            nylon         big             4\n     Soak_time_Or Catch_g\n1699            3  130933\n\n#stockC[1532,]\n\nstockC<-stockC[-1699,]   #remove outlier from the dataset, and check how results have changed"
  },
  {
    "objectID": "scripts/Hilsha.html",
    "href": "scripts/Hilsha.html",
    "title": "R code for hilsa (Tenualosa ilisha) CPUE data: zero inflated models",
    "section": "",
    "text": "Note that we are using a data file that has already been explored and modified to remove outliers. Read the introduction to the model to see where to find the original data file and explanation on data exploration"
  },
  {
    "objectID": "scripts/Hilsha.html#plot-model-outputs-and-parameters",
    "href": "scripts/Hilsha.html#plot-model-outputs-and-parameters",
    "title": "R code for hilsa (Tenualosa ilisha) CPUE data: zero inflated models",
    "section": "Plot model outputs and parameters",
    "text": "Plot model outputs and parameters\nWe plot model outputs to show predictions on how catches will depend on the number of days spent fishing and gillnet lengths.\n\n\n\n\n\n\n\n\nWe can also plot how catches depend on the trip length during different months and in different fishing areas."
  },
  {
    "objectID": "scripts/Hilsha.html#plot-model-parameter-estimates-in-a-publication-format",
    "href": "scripts/Hilsha.html#plot-model-parameter-estimates-in-a-publication-format",
    "title": "R code for hilsa (Tenualosa ilisha) CPUE data: zero inflated models",
    "section": "Plot model parameter estimates in a publication format",
    "text": "Plot model parameter estimates in a publication format\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n\n\n\n \nNB GLM (Hilsha)\n\n\nCoeffcient\nLog-Mean\nConf. Int (95%)\nP-value\n\n\n(Intercept)\n2.49\n2.06 – 2.92\n<0.001\n\n\nArea [LowerMeghna]\n-0.11\n-0.48 – 0.26\n0.556\n\n\nArea [LowerPadma]\n-0.44\n-0.89 – 0.01\n0.056\n\n\nYrMonth [Oct92]\n0.38\n0.10 – 0.66\n0.007\n\n\nYrMonth [Sep92]\n-0.08\n-0.35 – 0.20\n0.596\n\n\nNlength\n0.00\n-0.00 – 0.00\n0.174\n\n\nTripDays\n0.41\n0.33 – 0.49\n<0.001\n\n\nObservations\n266\n\n\nR2 conditional / R2 marginal\nNA / 0.960"
  },
  {
    "objectID": "scripts/JabbaModel_example.html",
    "href": "scripts/JabbaModel_example.html",
    "title": "JABBA example",
    "section": "",
    "text": "libraries\nJABBA requires the installation of R and JAGS and the following R packages that can be directly installed within R\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n\n\n\n### DATA sets\n\n\nall data\nJABBA requires a minimum of two input comma-separated value files (.csv) in the form of catch and abundance indices. The Catch input file contains the time series of year and catch by weight, aggregated across fleets for the entire fishery. Missing catch years or catch values are not allowed. JABBA is formulated to accommodate abundance indices from multiple sources (i.e., fleets) in a single cpue file, which contains all considered abundance indices. The first column of the cpue input is year, which must match the range of years provided in the Catch file. In contrast to the Catch input, missing abundance index values are allowed, such that different abundance indices may correspond to smaller portions of the catch time series. Optionally, an additional se input can be passed onto JABBA, containing standard error estimates associated with the abundance indices on a log scale. The se input is a third file, structurally identical to the cpue input. Alternatively, this feature can be used to apply different weighting to individual abundance indices by assigning varying coefficients of variation (CV) to each time series. If such weighting is implemented, it is advised that the CV chosen for each indexed year approximates the observed standard error on the log scale, such that the data weights are congruent with expectations as to how well the model should fit these data.\nJABBA provides provides the option to use a single averaged CPUE index instead of the individual abundance indices (see 2.5.1. State-Space model for averaging of abundance indices in Winker et al, 2018). This feature can be activated by setting meanCPUE = TRUE.\n\n\n  Year Species Total_catch  cpue_sc   cpue_SE\n1 1992 StockA1      829.20 1.798236        NA\n2 1993 StockA1      567.97 1.075917 0.5905036\n3 1994 StockA1      402.71 2.685084 0.5794874\n4 1995 StockA1      366.71 1.767176 0.5776929\n5 1996 StockA1      447.09 1.871632 0.5793944\n6 1997 StockA1      685.99 2.576474 0.5789248\n\n\n[1] \"StockA1\"\n\n\n#StockA1\n\nspecies <- \"StockA1\"\n\n## select one species\ndata <- all_data %>% filter (Species == species)\n\n#indicate catch, cpue and SE of cpue\ncatch<-data %>% dplyr::select (Year, Total_catch)\n\ncpue <- data %>% dplyr::select(Year, cpue_sc)\nse <- data %>% dplyr::select(Year, cpue_SE)\n\ncolnames(catch) <- c(\"year\", \"catch\")\ncolnames(cpue) <- c(\"year\", \"cpue\")\n\ncolnames(se) <- c(\"year\", \"se\")\n\nplot(catch)\n\n\n\nplot(cpue)\n\n\n\nplot(se)\n\n\n\n\n\n\n### SETUP AND RUN\n\n\nSetup model parameters\nFor surplus production models we basically need carrying capacity K and population growth rate r Since K and r are never known, a common approach is to set priors for K at 3-10 times the maximum catch in the catch time series, whereas r is assumed to be lognormally distributed with mean r of 0.2. But you can set it differently. Let’s also make projections into the future assuming that catch in the next 10 years will stay the same as the last year’s catch, be 50% smaller and 50% larger\n\n\n[1] 1012.672\n\n\n\nsetup: Schaeffer\n\nscenario_name = \"Schaeffer_both\"\n\ninput_both_sch <- build_jabba(\n  catch = catch,\n  cpue = cpue,\n  se = NULL,\n  assessment = species,\n  scenario = scenario_name,\n  model.type = c(\"Schaefer\", \"Fox\", \"Pella\", \"Pella_m\") [1],  # with [1] we select the first option\n  add.catch.CV = TRUE,\n  catch.cv = 0.1,\n  catch.error = c(\"random\", \"under\") [1],          #\"under\" mean directional underreporting\n  Plim = 0.25,                                    #limit of biomass where recruitment can become impaired Plim = Blim/K\n  r.dist = c(\"lnorm\", \"range\")[1],\n  r.prior = c(0.4, 0.2),             #IMPORTANT: What is the expected mean R and lognormal errors? Default is 0.2 and 0.25\n  K.dist = c(\"lnorm\", \"range\")[2],\n  K.prior = c(Kmin,Kmax),\n  psi.dist = c(\"lnorm\", \"beta\")[1],\n  psi.prior = c(0.6, 0.2),\n  b.prior = c(FALSE, 0.3, NA, c(\"bk\", \"bbmsy\", \"ffmsy\")[1]),\n  BmsyK = 0.4,\n  shape.CV = 0.3,\n  sets.q = 1:(ncol(cpue) - 1),\n  sigma.est = TRUE,\n  sets.var = 1:(ncol(cpue) - 1),\n  fixed.obsE = 0.01,\n  #fixed.obsE = ifelse(is.null(se), 0.1, 0.001),\n  sigma.proc = TRUE,\n  proc.dev.all = TRUE,\n  igamma = c(4, 0.01),\n  projection = TRUE,\n  TACs = TAC_range, # we set a range in here\n  #TACs = c(350,1000), # optionally we can set a range in here\n  TACint = NULL,\n  imp.yr = TAC_year,        #when does TAC start\n  pyrs = 10,            #how many years to project     \n  P_bound = c(0.02, 1.3),\n  sigmaobs_bound = 1,\n  sigmaproc_bound = 0.2,\n  q_bounds = c(10^-30, 1000),\n  K_bounds = c(0.01, 10^10),\n  harvest.label = c(\"Hmsy\", \"Fmsy\")[2],\n  catch.metric = \"(t)\"\n)\n\n\n><> Prepare JABBA input data <><\n\n\n\n><> Assume Catch with error CV = 0.1 <><\n\n\n\n><> SE.I=FALSE: Creating cpue.se dummy matrix <><\n\n\n\n><> Model type:Schaefer <><\n\n\n\n><> Shape m =2\n\n\n\n><> K prior mean =5803.66448900219and CV =0.307940901904413(log.sd = 0.300993201081484)\n\n\n\n><> r prior mean =0.4and CV =0.20201676710706(log.sd = 0.2)\n\n\n\n><> Psi (B1/K) prior mean =0.6and CV =0.2withlnormdestribution\n\n\n\n\n\n><> ALWAYS ENSURE to adjust default settings to your specific stock <><\n\n\n\n\nSchaefer with CPUE SE\n\nscenario_name = \"Schaeffer_both\"\n\ninput_both_sch <- build_jabba(\n  catch = catch,\n  cpue = cpue,\n  se = se,\n  assessment = species,\n  scenario = scenario_name,\n  model.type = c(\"Schaefer\", \"Fox\", \"Pella\", \"Pella_m\") [1],  # with [1] we select the first option\n  add.catch.CV = TRUE,\n  catch.cv = 0.1,\n  catch.error = c(\"random\", \"under\") [1],          #\"under\" mean directional underreporting\n  Plim = 0.25,                                    #limit of biomass where recruitment can become impaired Plim = Blim/K\n  r.dist = c(\"lnorm\", \"range\")[1],\n  r.prior = c(0.4, 0.2),             #IMPORTANT: What is the expected mean R and lognormal errors? Default is 0.2 and 0.25\n  K.dist = c(\"lnorm\", \"range\")[2],\n  K.prior = c(Kmin,Kmax),\n  psi.dist = c(\"lnorm\", \"beta\")[1],\n  psi.prior = c(0.6, 0.2),\n  b.prior = c(FALSE, 0.3, NA, c(\"bk\", \"bbmsy\", \"ffmsy\")[1]),\n  BmsyK = 0.4,\n  shape.CV = 0.3,\n  sets.q = 1:(ncol(cpue) - 1),\n  sigma.est = TRUE,\n  sets.var = 1:(ncol(cpue) - 1),\n  #fixed.obsE = 0.01,\n  fixed.obsE = ifelse(is.null(se), 0.1, 0.001),\n  sigma.proc = TRUE,\n  proc.dev.all = TRUE,\n  igamma = c(4, 0.01),\n  projection = TRUE,\n  TACs = TAC_range, # we set a range in here\n  #TACs = c(350,1000), # we set a range in here\n  TACint = NULL,\n  imp.yr = TAC_year,        #when does TAC start\n  pyrs = 10,            #how many years to project     \n  P_bound = c(0.02, 1.3),\n  sigmaobs_bound = 1,\n  sigmaproc_bound = 0.2,\n  q_bounds = c(10^-30, 1000),\n  K_bounds = c(0.01, 10^10),\n  harvest.label = c(\"Hmsy\", \"Fmsy\")[2],\n  catch.metric = \"(t)\"\n)\n\n\n><> Prepare JABBA input data <><\n\n\n\n><> Assume Catch with error CV = 0.1 <><\n\n\n\n><> Model type:Schaefer <><\n\n\n\n><> Shape m =2\n\n\n\n><> K prior mean =5803.66448900219and CV =0.307940901904413(log.sd = 0.300993201081484)\n\n\n\n><> r prior mean =0.4and CV =0.20201676710706(log.sd = 0.2)\n\n\n\n><> Psi (B1/K) prior mean =0.6and CV =0.2withlnormdestribution\n\n\n\n\n\n><> ALWAYS ENSURE to adjust default settings to your specific stock <><\n\n\n\n\nsetup: Fox\n\nscenario_name = \"Fox_both\"\n\ninput_both_fox <- build_jabba(\n  catch = catch,\n  cpue = cpue,\n  se = NULL,\n  assessment = species,\n  scenario = scenario_name,\n  model.type = c(\"Schaefer\", \"Fox\", \"Pella\", \"Pella_m\") [2],  # with [1] we select the first option\n  add.catch.CV = TRUE,\n  catch.cv = 0.1,\n  catch.error = c(\"random\", \"under\") [1],          #\"under\" mean directional underreporting\n  Plim = 0.25,                                    #limit of biomass where recruitment can become impaired Plim = Blim/K\n  r.dist = c(\"lnorm\", \"range\")[1],\n  r.prior = c(0.25, 0.2),             #IMPORTANT: What is the expected mean R and lognormal errors? Default is 0.2 and 0.25\n  K.dist = c(\"lnorm\", \"range\")[2],\n  K.prior = c(Kmin,Kmax),\n  psi.dist = c(\"lnorm\", \"beta\"),\n  psi.prior = c(0.6, 0.2),\n  b.prior = c(FALSE, 0.3, NA, c(\"bk\", \"bbmsy\", \"ffmsy\")[1]),\n  BmsyK = 0.4,\n  shape.CV = 0.3,\n  sets.q = 1:(ncol(cpue) - 1),\n  sigma.est = TRUE,\n  sets.var = 1:(ncol(cpue) - 1),\n  fixed.obsE = 0.01,\n  #fixed.obsE = ifelse(is.null(se), 0.1, 0.001),\n  sigma.proc = TRUE,\n  proc.dev.all = TRUE,\n  igamma = c(4, 0.01),\n  projection = TRUE,\n  TACs = TAC_range, # we set a range in here\n  #TACs = c(350,1000), # we set a range in here\n  TACint = NULL,\n  imp.yr = TAC_year,        #when does TAC start\n  pyrs = 10,            #how many years to project     \n  P_bound = c(0.02, 1.3),\n  sigmaobs_bound = 1,\n  sigmaproc_bound = 0.2,\n  q_bounds = c(10^-30, 1000),\n  K_bounds = c(0.01, 10^10),\n  harvest.label = c(\"Hmsy\", \"Fmsy\")[2],\n  catch.metric = \"(t)\"\n)\n\n\n><> Prepare JABBA input data <><\n\n\n\n><> Assume Catch with error CV = 0.1 <><\n\n\n\n><> SE.I=FALSE: Creating cpue.se dummy matrix <><\n\n\n\n><> Model type:Fox <><\n\n\n\n><> Shape m =1.001\n\n\n\n><> K prior mean =5803.66448900219and CV =0.307940901904413(log.sd = 0.300993201081484)\n\n\n\n><> r prior mean =0.25and CV =0.20201676710706(log.sd = 0.2)\n\n\n\n><> Psi (B1/K) prior mean =0.6and CV =0.2withlnormdestribution\n\n\n\n\n\n><> ALWAYS ENSURE to adjust default settings to your specific stock <><\n\n\n\n\nFox with CPUE SE\n\nscenario_name = \"Fox_both\"\n\ninput_both_fox <- build_jabba(\n  catch = catch,\n  cpue = cpue,\n  se = se,\n  assessment = species,\n  scenario = scenario_name,\n  model.type = c(\"Schaefer\", \"Fox\", \"Pella\", \"Pella_m\") [2],  # with [1] we select the first option\n  add.catch.CV = TRUE,\n  catch.cv = 0.1,\n  catch.error = c(\"random\", \"under\") [1],          #\"under\" mean directional underreporting\n  Plim = 0.25,                                    #limit of biomass where recruitment can become impaired Plim = Blim/K\n  r.dist = c(\"lnorm\", \"range\")[1],\n  r.prior = c(0.25, 0.2),             #IMPORTANT: What is the expected mean R and lognormal errors? Default is 0.2 and 0.25\n  K.dist = c(\"lnorm\", \"range\")[2],\n  K.prior = c(Kmin,Kmax),\n  psi.dist = c(\"lnorm\", \"beta\"),\n  psi.prior = c(0.6, 0.2),\n  b.prior = c(FALSE, 0.3, NA, c(\"bk\", \"bbmsy\", \"ffmsy\")[1]),\n  BmsyK = 0.4,\n  shape.CV = 0.3,\n  sets.q = 1:(ncol(cpue) - 1),\n  sigma.est = TRUE,\n  sets.var = 1:(ncol(cpue) - 1),\n  #fixed.obsE = 0.01,\n  fixed.obsE = ifelse(is.null(se), 0.1, 0.001),\n  sigma.proc = TRUE,\n  proc.dev.all = TRUE,\n  igamma = c(4, 0.01),\n  projection = TRUE,\n  TACs = TAC_range, # we set a range in here\n  #TACs = c(350,1000), # we set a range in here\n  TACint = NULL,\n  imp.yr = TAC_year,        #when does TAC start\n  pyrs = 10,            #how many years to project     \n  P_bound = c(0.02, 1.3),\n  sigmaobs_bound = 1,\n  sigmaproc_bound = 0.2,\n  q_bounds = c(10^-30, 1000),\n  K_bounds = c(0.01, 10^10),\n  harvest.label = c(\"Hmsy\", \"Fmsy\")[2],\n  catch.metric = \"(t)\"\n)\n\n\n><> Prepare JABBA input data <><\n\n\n\n><> Assume Catch with error CV = 0.1 <><\n\n\n\n><> Model type:Fox <><\n\n\n\n><> Shape m =1.001\n\n\n\n><> K prior mean =5803.66448900219and CV =0.307940901904413(log.sd = 0.300993201081484)\n\n\n\n><> r prior mean =0.25and CV =0.20201676710706(log.sd = 0.2)\n\n\n\n><> Psi (B1/K) prior mean =0.6and CV =0.2withlnormdestribution\n\n\n\n\n\n><> ALWAYS ENSURE to adjust default settings to your specific stock <><\n\n\n\n\nsetup: Pella\n\nscenario_name = \"Pella_both\"\n\ninput_both_pella <- build_jabba(\n  catch = catch,\n  cpue = cpue,\n  se = NULL,\n  assessment = species,\n  scenario = scenario_name,\n  model.type = c(\"Schaefer\", \"Fox\", \"Pella\", \"Pella_m\") [3],  # with [1] we select the first option\n  add.catch.CV = TRUE,\n  catch.cv = 0.1,\n  catch.error = c(\"random\", \"under\") [1],          #\"under\" mean directional underreporting\n  Plim = 0.25,                                    #limit of biomass where recruitment can become impaired Plim = Blim/K\n  r.dist = c(\"lnorm\", \"range\")[1],\n  r.prior = c(0.25, 0.2),             #IMPORTANT: What is the expected mean R and lognormal errors? Default is 0.2 and 0.25\n  K.dist = c(\"lnorm\", \"range\")[2],\n  K.prior = c(Kmin,Kmax),\n  psi.dist = c(\"lnorm\", \"beta\"),\n  psi.prior = c(0.6, 0.2),\n  b.prior = c(FALSE, 0.3, NA, c(\"bk\", \"bbmsy\", \"ffmsy\")[1]),\n  BmsyK = 0.4,\n  shape.CV = 0.3,\n  sets.q = 1:(ncol(cpue) - 1),\n  sigma.est = TRUE,\n  sets.var = 1:(ncol(cpue) - 1),\n  fixed.obsE = 0.01,\n  #fixed.obsE = ifelse(is.null(se), 0.1, 0.001),\n  sigma.proc = TRUE,\n  proc.dev.all = TRUE,\n  igamma = c(4, 0.01),\n  projection = TRUE,\n  TACs = TAC_range, # we set a range in here\n  #TACs = c(350,1000), # we set a range in here\n  TACint = NULL,\n  imp.yr = TAC_year,        #when does TAC start\n  pyrs = 10,            #how many years to project     \n  P_bound = c(0.02, 1.3),\n  sigmaobs_bound = 1,\n  sigmaproc_bound = 0.2,\n  q_bounds = c(10^-30, 1000),\n  K_bounds = c(0.01, 10^10),\n  harvest.label = c(\"Hmsy\", \"Fmsy\")[2],\n  catch.metric = \"(t)\"\n)\n\n\n><> Prepare JABBA input data <><\n\n\n\n><> Assume Catch with error CV = 0.1 <><\n\n\n\n><> SE.I=FALSE: Creating cpue.se dummy matrix <><\n\n\n\n><> Model type:Pella <><\n\n\n\n><> Shape m =1.188\n\n\n\n><> K prior mean =5803.66448900219and CV =0.307940901904413(log.sd = 0.300993201081484)\n\n\n\n><> r prior mean =0.25and CV =0.20201676710706(log.sd = 0.2)\n\n\n\n><> Psi (B1/K) prior mean =0.6and CV =0.2withlnormdestribution\n\n\n\n\n\n><> ALWAYS ENSURE to adjust default settings to your specific stock <><\n\n\n\n\nPella with CPUE SE\n\nscenario_name = \"Pella_both\"\n\ninput_both_pella <- build_jabba(\n  catch = catch,\n  cpue = cpue,\n  se = se,\n  assessment = species,\n  scenario = scenario_name,\n  model.type = c(\"Schaefer\", \"Fox\", \"Pella\", \"Pella_m\") [3],  # with [1] we select the first option\n  add.catch.CV = TRUE,\n  catch.cv = 0.1,\n  catch.error = c(\"random\", \"under\") [1],          #\"under\" mean directional underreporting\n  Plim = 0.25,                                    #limit of biomass where recruitment can become impaired Plim = Blim/K\n  r.dist = c(\"lnorm\", \"range\")[1],\n  r.prior = c(0.25, 0.2),             #IMPORTANT: What is the expected mean R and lognormal errors? Default is 0.2 and 0.25\n  K.dist = c(\"lnorm\", \"range\")[2],\n  K.prior = c(Kmin,Kmax),\n  psi.dist = c(\"lnorm\", \"beta\"),\n  psi.prior = c(0.6, 0.2),\n  b.prior = c(FALSE, 0.3, NA, c(\"bk\", \"bbmsy\", \"ffmsy\")[1]),\n  BmsyK = 0.4,\n  shape.CV = 0.3,\n  sets.q = 1:(ncol(cpue) - 1),\n  sigma.est = TRUE,\n  sets.var = 1:(ncol(cpue) - 1),\n  #fixed.obsE = 0.01,\n  fixed.obsE = ifelse(is.null(se), 0.1, 0.001),\n  sigma.proc = TRUE,\n  proc.dev.all = TRUE,\n  igamma = c(4, 0.01),\n  projection = TRUE,\n  TACs = TAC_range, # we set a range in here\n  #TACs = c(350,1000), # we set a range in here\n  TACint = NULL,\n  imp.yr = TAC_year,        #when does TAC start\n  pyrs = 10,            #how many years to project     \n  P_bound = c(0.02, 1.3),\n  sigmaobs_bound = 1,\n  sigmaproc_bound = 0.2,\n  q_bounds = c(10^-30, 1000),\n  K_bounds = c(0.01, 10^10),\n  harvest.label = c(\"Hmsy\", \"Fmsy\")[2],\n  catch.metric = \"(t)\"\n)\n\n\n><> Prepare JABBA input data <><\n\n\n\n><> Assume Catch with error CV = 0.1 <><\n\n\n\n><> Model type:Pella <><\n\n\n\n><> Shape m =1.188\n\n\n\n><> K prior mean =5803.66448900219and CV =0.307940901904413(log.sd = 0.300993201081484)\n\n\n\n><> r prior mean =0.25and CV =0.20201676710706(log.sd = 0.2)\n\n\n\n><> Psi (B1/K) prior mean =0.6and CV =0.2withlnormdestribution\n\n\n\n\n\n><> ALWAYS ENSURE to adjust default settings to your specific stock <><\n\n\n#RUN. Make sure you have created a separate folder with species name, so that jabba would know where to put results\n\n\n\nRun: Schaeffer\nWe will run Schaefer, Fox and Pella scenarios. They make different assumptions about density dependence. We run them with identical parameters and priors, but we expect to get different posterior estimates, because they assume a slightly different production curve\n\n# creating a directory to save the output files\ndir.create(species) \n\n\n#note this will save an .RData object \n\nrun_both_sch <- fit_jabba(\n  input_both_sch,\n  ni = 30000,\n  nt = 5,\n  nb = 5000,\n  nc = 2,\n  init.values = FALSE,\n  init.K = NULL,\n  init.r = NULL,\n  init.q = NULL,\n  peels = NULL,\n  save.all = FALSE,\n  save.trj = TRUE,\n  save.jabba = TRUE,  #will save the RData object\n  save.csvs = TRUE,\n  output.dir = species,  ## make sure you create a folder in your working directory to save outputs\n  quickmcmc = TRUE\n)\n\nmodule glm loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 92\n   Unobserved stochastic nodes: 128\n   Total graph size: 2350\n\nInitializing model\n\n\n\n><> Produce results output of Schaefer model for StockA1 Schaeffer_both <><\n\n\n\n\n><> Scenario Schaeffer_both_Schaefer completed in 0 min and 37 sec <><\n\n\n\n><> compiling Future Projections under fixed quota <><\n\n\n\n\nRun: Fox\n\nrun_both_fox <- fit_jabba(\n  input_both_fox,\n  ni = 30000,\n  nt = 5,\n  nb = 5000,\n  nc = 2,\n  init.values = FALSE,\n  init.K = NULL,\n  init.r = NULL,\n  init.q = NULL,\n  peels = NULL,\n  save.all = FALSE,\n  save.trj = TRUE,\n  save.jabba = TRUE,  #will save the RData object\n  save.csvs = TRUE,\n  output.dir = species,  ## make sure you create a folder in your working directory to save outputs\n  quickmcmc = TRUE\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 92\n   Unobserved stochastic nodes: 128\n   Total graph size: 2350\n\nInitializing model\n\n\n\n><> Produce results output of Fox model for StockA1 Fox_both <><\n\n\n\n\n><> Scenario Fox_both_Fox completed in 0 min and 30 sec <><\n\n\n\n><> compiling Future Projections under fixed quota <><\n\n\n\n\nRun: Pella\n\nrun_both_pella <- fit_jabba(\n  input_both_pella,\n  ni = 30000,\n  nt = 5,\n  nb = 5000,\n  nc = 2,\n  init.values = FALSE,\n  init.K = NULL,\n  init.r = NULL,\n  init.q = NULL,\n  peels = NULL,\n  save.all = FALSE,\n  save.trj = TRUE,\n  save.jabba = TRUE,  #will save the RData object\n  save.csvs = TRUE,\n  output.dir = species,  ## make sure you create a folder in your working directory to save outputs\n  quickmcmc = TRUE\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 92\n   Unobserved stochastic nodes: 128\n   Total graph size: 2350\n\nInitializing model\n\n\n\n><> Produce results output of Pella model for StockA1 Pella_both <><\n\n\n\n\n><> Scenario Pella_both_Pella completed in 0 min and 33 sec <><\n\n\n\n><> compiling Future Projections under fixed quota <><\n\n\n\n\n### MAKE OUTPUTS\n\n\nall output plots into directory\nYou can explore these plots from the working directory\n\njabba_plots(run_both_sch, output.dir = species)\n\n\n><> jbplot_catch()  <><\n\n\n\n><> jbplot_catcherror()  <><\n\n\n\n><> jbplot_cpue() - fits to CPUE <><\n\n\n\n><> jbplot_cpue() - fits to CPUE <><\n\n\n\n><> jbplot_mcmc() - mcmc chains  <><\n\n\n\n><> jbplot_ppist() - prior and posterior distributions  <><\n\n\n\n><> jbplot_procdev() - Process error diviations on log(biomass)  <><\n\n\n\n><> jbplot_trj() - B trajectory  <><\n\n\n\n><> jbplot_trj() - F trajectory  <><\n\n\n\n><> jbplot_trj() - BBmsy trajectory  <><\n\n\n\n><> jbplot_trj() - FFmsy trajectory  <><\n\n\n\n><> jbplot_trj() - BB0 trajectory  <><\n\n\n\n><> jbplot_spphase() - JABBA Surplus Production Phase Plot  <><\n\n\n\n><> jbplot_cpue() - fits to CPUE <><\n\n\n\n><> jbplot_runstest()   <><\n\n\n\n><> jbplot_summary()\n\n\n\n><> jbplot_kobe() - Stock Status Plot  <><\n\n\n\n\n\npng \n  2 \n\n#jabba_plots(run_both_fox, output.dir = species)\n#jabba_plots(run_both_pella, output.dir = species)\n\n\n\nsummary plot in pdf\nIf you want to generate your summary plot\n\n#load the data file, but it is always saved as name \"jabba\". Chose which file you want: Schaeffer, Fox or Pella\n\n load(file = paste0(species,\"/\",species,\"_Schaeffer_both_jabba.rdata\"))\n#load(file = paste0(species,\"/\",species,\"_Fox_both_jabba.rdata\"))\n# load(file = paste0(species,\"/\",species,\"_Pella_both_jabba.rdata\"))\n\n #rename it, to not get confused\n model_out <- jabba\n\n #make a name for a summary pdf file \npdf(file = paste0(species,\"/\",species,\"_summary.pdf\"), width = 11, height = 12)\n\n# plot various outputs \n\npar(mfrow=c(3,2),mar = c(5, 5, 4, 3))\njbplot_trj(model_out,type=\"BBmsy\",add=T)\n\n\n><> jbplot_trj() - BBmsy trajectory  <><\n\njbplot_trj(model_out,type=\"F\",add=T)\n\n\n><> jbplot_trj() - F trajectory  <><\n\njbplot_catch(model_out, add = T)\n\n\n><> jbplot_catch()  <><\n\njbplot_spphase(model_out,add=T)\n\n\n><> jbplot_spphase() - JABBA Surplus Production Phase Plot  <><\n\njbplot_kobe(model_out, add = T)\n\n\n><> jbplot_kobe() - Stock Status Plot  <><\n\ndev.off()\n\npng \n  2 \n\n\n\n\n### END"
  },
  {
    "objectID": "surplus-production.html",
    "href": "surplus-production.html",
    "title": "Surplus production models",
    "section": "",
    "text": "Blah spm info\nHere is a link to the shiny app\n\n\nLithuanian language here\nHere is a link to the shiny app"
  },
  {
    "objectID": "temperature_growth.html",
    "href": "temperature_growth.html",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "",
    "text": "Fish growth is strongly dependent on temperature. In warmer conditions fish often growth faster as juveniles, mature at smaller sizes and are smaller as adults. This is known as the temperature size rule, as defined by David Atkinson in 1994.\n\n\n\n\n\nHowever, the mechanisms behind this growth remain unclear. In this model we propose that temperature size rule (TSR) emerges in response to both physiological changes (faster metabolism and intake) and growth and reproduction optimisation to changes in mortality. We develop a physiologically structured life history optimisation model and explore a range of parameters and scenarios under which TSR is likely to emerge. For a brief introduction into the model you can watch these slides or watch this conference talk."
  },
  {
    "objectID": "temperature_growth.html#model-code",
    "href": "temperature_growth.html#model-code",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "Model code",
    "text": "Model code\nThe model is written in Microsoft Excel and R environment. All model code and details are available on this GitHub repository. The main model Excel file can also be downloaded here. In this Excel file you can modify temperature response parameters and their size dependency, optimise the model using Solver option built in Excel and see the resulting growth and reproduction curves, as shown in the figure below."
  },
  {
    "objectID": "temperature_growth.html#application-of-the-model",
    "href": "temperature_growth.html#application-of-the-model",
    "title": "Model to explore temperature impacts on fish growth",
    "section": "Application of the model",
    "text": "Application of the model\nThe open access publication presenting this model is published in a special issue of The Biological Bulletin. If you want to learn more about the role of temperature and oxygen on fish growth, please also check out this overview publication in the same special issue.\nThis model is easy to modify and explore, and you are welcome to use it in your research and teaching. If you would like to suggest or implement new model modifications and publish them, please contact us at lydekaipaliepus@gamtc.lt\n Vendace drawing by Amy Rose Coghlan"
  },
  {
    "objectID": "to_do_list.html",
    "href": "to_do_list.html",
    "title": "TO DO list",
    "section": "",
    "text": "Why is our webiste https://fishsizeproject.github.io/models/ not updated once I push code updates? I can see them locally, but not online"
  },
  {
    "objectID": "to_do_list.html#section",
    "href": "to_do_list.html#section",
    "title": "TO DO list",
    "section": "",
    "text": "Asta: get chlorophyl A image"
  },
  {
    "objectID": "von-bertalanffy.html",
    "href": "von-bertalanffy.html",
    "title": "von Bertalanffy Growth models",
    "section": "",
    "text": "von Bert info (english)\nHere is a link to the shiny app\n\n\nvon Bert info (lithuanian)\nHere is a link to the shiny app"
  },
  {
    "objectID": "Zero_inflated_model.html",
    "href": "Zero_inflated_model.html",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "",
    "text": "Most fisheries datasets (scientific, commercial, recreational) have a lot of zero catches. These are fishing trips were no fish was caught. These zero catches are important but we need to fit appropriate models to accommodate them. Here we are introducing a model for zero inflated data. This model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling.\nFor a basic introduction into the model and data, check out these slides. However, if you want to use the model we strongly recommend that you watch at least part 4 of our CPUE standardisation course, where the model and approach were presented in more detail.\nBefore proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage."
  },
  {
    "objectID": "Zero_inflated_model.html#lithintroduction",
    "href": "Zero_inflated_model.html#lithintroduction",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "LITH:Introduction",
    "text": "LITH:Introduction\nLITH:Most fisheries datasets (scientific, commercial, recreational) have a lot of zero catches. These are fishing trips were no fish was caught. These zero catches are important but we need to fit appropriate models to accommodate them. Here we are introducing a model for zero inflated data. This model is coded in R language, so you will need R and RStudio to run it. If you need a brief introduction into R, check this introductory page from an online course on size based modelling.\nFor a basic introduction into the model and data, check out these slides. However, if you want to use the model we strongly recommend that you watch at least part 4 of our CPUE standardisation course, where the model and approach were presented in more detail.\nBefore proceeding with CPUE standardisation, first make sure you carefully assess and explore your data. We have prepared two courses and different scripts for data exploration, which you can find on our CPUE standardisation course webpage."
  },
  {
    "objectID": "Zero_inflated_model.html#model-code",
    "href": "Zero_inflated_model.html#model-code",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "Model code",
    "text": "Model code\nThe main model code is available in this R markdown script and is demonstrated using this dataset. You can download the model and modify the script according to your needs. To look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted catches as a function of fishing time (or other estimate of effort) and month or season, like in the plot below."
  },
  {
    "objectID": "Zero_inflated_model.html#lithmodel-code",
    "href": "Zero_inflated_model.html#lithmodel-code",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "LITH:Model code",
    "text": "LITH:Model code\nLITH:The main model code is available in this R markdown script and is demonstrated using this dataset. You can download the model and modify the script according to your needs. To look at the model code and outputs without having to run the code, you can click here. After applying the model you should be able to plot predicted catches as a function of fishing time (or other estimate of effort) and month or season, like in the plot below."
  },
  {
    "objectID": "Zero_inflated_model.html#application-of-the-model",
    "href": "Zero_inflated_model.html#application-of-the-model",
    "title": "Zero inflated model for CPUE standardisation",
    "section": "Application of the model",
    "text": "Application of the model\nTo better understand this model and its applications, we strongly recommend that you go through our CPUE standardisation course material, where we discuss different models and their strengths in greater detail. The course also explains how to simulate new datasets using estimated model parameters to assess the probability of obtaining as many zero entries as you have in your dataset (example output of these simulations is shown in the plot below)."
  }
]